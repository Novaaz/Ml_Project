{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e3267e",
   "metadata": {},
   "source": [
    "# Unified HCF+PCA+SVM Pipeline: Clean Features vs Degraded Test\n",
    "Comprehensive comparison: Color Histogram solo, then with HOG/LBP/GABOR/GLCM.\n",
    "\n",
    "**Key aspects:**\n",
    "- Training augmentation: 20% scenario-based (A/B/C with weights 0.4/0.4/0.2)\n",
    "- Test evaluation: Mixed degradation (60% clean, 15% A, 15% B, 10% C)\n",
    "- Grid search: Subset-based with cross-validation\n",
    "- Validation: Proper hold-out split\n",
    "- Models saved as joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fa2ad7",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1024c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b35ed",
   "metadata": {},
   "source": [
    "## 2. Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ebe9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Image size: 32x32\n",
      "  Training augmentation: 20.0%\n",
      "  Test degradation: [0.6, 0.15, 0.15, 0.1]\n",
      "  Histogram bins: 32\n",
      "  PCA variance targets: [0.95]\n",
      "  Grid search: C=[10, 100], gamma=[0.01, 0.001]\n",
      "  Models saved to: saved_models/hcf_degraded_comparison\n"
     ]
    }
   ],
   "source": [
    "# Random state\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Image processing\n",
    "SIZE = 32\n",
    "VARIETY = False  # False = coarse labels (Apple, not Apple Braeburn)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Training augmentation\n",
    "AUG_RATIO = 0.20  # 20% of training set will be augmented\n",
    "AUG_DIST = {\"A\": 0.4, \"B\": 0.4, \"C\": 0.2}  # Scenario distribution\n",
    "\n",
    "# Feature extraction\n",
    "HIST_BINS = 32\n",
    "\n",
    "# Grid search parameters\n",
    "VARIANCE_TARGETS = [0.95]\n",
    "CV_FOLDS = 3\n",
    "TUNING_SUBSET_RATIO = 0.20\n",
    "C_VALUES = [10, 100]\n",
    "GAMMA_VALUES = [0.01, 0.001]\n",
    "\n",
    "# Test set degradation probabilities\n",
    "TEST_DEGRADATION_DISTRIBUTION = [0.60, 0.15, 0.15, 0.10]  # clean, A, B, C\n",
    "\n",
    "# Model save directory\n",
    "MODEL_SAVE_DIR = Path(\"saved_models/hcf_degraded_comparison\")\n",
    "MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Image size: {SIZE}x{SIZE}\")\n",
    "print(f\"  Training augmentation: {AUG_RATIO*100}%\")\n",
    "print(f\"  Test degradation: {TEST_DEGRADATION_DISTRIBUTION}\")\n",
    "print(f\"  Histogram bins: {HIST_BINS}\")\n",
    "print(f\"  PCA variance targets: {VARIANCE_TARGETS}\")\n",
    "print(f\"  Grid search: C={C_VALUES}, gamma={GAMMA_VALUES}\")\n",
    "print(f\"  Models saved to: {MODEL_SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fdb7d8",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59518eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: dataset/fruit360\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"dataset/fruit360\"\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"Training\")\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"Test\")\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/fruits-360/fruits-360-100x100\"\n",
    "CLONE_DIR = \"dataset/fruits-360-100x100\"\n",
    "\n",
    "def download_dataset():\n",
    "    os.makedirs(\"dataset\", exist_ok=True)\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, CLONE_DIR], check=True)\n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Training\"), TRAIN_DIR)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Test\"), TEST_DIR)\n",
    "    shutil.rmtree(CLONE_DIR, ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    download_dataset()\n",
    "\n",
    "assert os.path.exists(TRAIN_DIR), f\"{TRAIN_DIR} not found\"\n",
    "assert os.path.exists(TEST_DIR), f\"{TEST_DIR} not found\"\n",
    "print(f\"Dataset ready: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb186f1",
   "metadata": {},
   "source": [
    "## 4. Dataset Class & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63459fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 130344 images, 79 classes\n",
      "Test: 43442 images, 79 classes\n",
      "\n",
      "Data split: Train 91240, Val 39104, Test 43442\n",
      "Classes: 79\n"
     ]
    }
   ],
   "source": [
    "class Fruit360Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, variety=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.variety = variety\n",
    "        self.samples = []\n",
    "\n",
    "        for class_name in sorted(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            label = class_name if variety else class_name.split()[0]\n",
    "\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith((\".jpg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), label))\n",
    "\n",
    "        self.labels = sorted({lbl for _, lbl in self.samples})\n",
    "        self.label_to_idx = {lbl: i for i, lbl in enumerate(self.labels)}\n",
    "        self.idx_to_label = {i: lbl for lbl, i in self.label_to_idx.items()}\n",
    "\n",
    "        print(f\"{os.path.basename(root_dir)}: {len(self.samples)} images, {len(self.labels)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label_idx = self.label_to_idx[label_str]\n",
    "        return img, label_idx\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "transform = T.Compose([T.Resize((SIZE, SIZE)), T.ToTensor()])\n",
    "\n",
    "train_full = Fruit360Dataset(TRAIN_DIR, transform=transform, variety=VARIETY)\n",
    "test_dataset = Fruit360Dataset(TEST_DIR, transform=transform, variety=VARIETY)\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.7 * len(train_full))\n",
    "val_size = len(train_full) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_full,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_STATE),\n",
    ")\n",
    "\n",
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nData split: Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "print(f\"Classes: {len(train_full.labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c23e7",
   "metadata": {},
   "source": [
    "## 5. Convert Loaders to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a727a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: X_train (91240, 3, 32, 32), X_val (39104, 3, 32, 32), X_test (43442, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "def extract_numpy(loader):\n",
    "    \"\"\"Extract images and labels from DataLoader to NumPy arrays.\"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    for imgs, labels in loader:\n",
    "        X_list.append(imgs.numpy())\n",
    "        y_list.append(labels.numpy())\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = extract_numpy(train_loader)\n",
    "X_val, y_val = extract_numpy(val_loader)\n",
    "X_test, y_test = extract_numpy(test_loader)\n",
    "\n",
    "print(f\"Extracted: X_train {X_train.shape}, X_val {X_val.shape}, X_test {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8995e",
   "metadata": {},
   "source": [
    "## 6. Augmentation Functions (Scenarios A/B/C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f09f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation scenarios defined: A (blur+noise+dirty), B (light+noise), C (occlusion+bruise)\n"
     ]
    }
   ],
   "source": [
    "def clamp_01(x):\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def add_color_patches(x, num_patches, color, alpha_range=(0.4, 0.7), size_range=(0.05, 0.15)):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    for _ in range(num_patches):\n",
    "        s = np.random.uniform(size_range[0], size_range[1])\n",
    "        patch_area = s * H * W / 4\n",
    "        r = np.random.uniform(0.5, 1.5)\n",
    "        patch_h = int(np.sqrt(patch_area / r))\n",
    "        patch_w = int(np.sqrt(patch_area * r))\n",
    "        patch_h = max(1, min(H, patch_h))\n",
    "        patch_w = max(1, min(W, patch_w))\n",
    "        top = np.random.randint(0, H - patch_h + 1)\n",
    "        left = np.random.randint(0, W - patch_w + 1)\n",
    "        bottom = top + patch_h\n",
    "        right = left + patch_w\n",
    "        alpha = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "        patch = out[:, top:bottom, left:right]\n",
    "        blended = alpha * color + (1 - alpha) * patch\n",
    "        out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "\n",
    "def add_occlusion_patch(x, area_ratio=0.1, color=torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1), alpha=0.5):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    patch_area = area_ratio * H * W\n",
    "    r = np.random.uniform(0.5, 1.5)\n",
    "    patch_h = int(np.sqrt(patch_area / r))\n",
    "    patch_w = int(np.sqrt(patch_area * r))\n",
    "    patch_h = max(1, min(H, patch_h))\n",
    "    patch_w = max(1, min(W, patch_w))\n",
    "    top = np.random.randint(0, H - patch_h + 1)\n",
    "    left = np.random.randint(0, W - patch_w + 1)\n",
    "    bottom = top + patch_h\n",
    "    right = left + patch_w\n",
    "    patch = out[:, top:bottom, left:right]\n",
    "    blended = alpha * color + (1 - alpha) * patch\n",
    "    out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "\n",
    "color_dirt = torch.tensor([0.3, 0.25, 0.2]).view(3, 1, 1)\n",
    "color_bruise = torch.tensor([0.25, 0.2, 0.15]).view(3, 1, 1)\n",
    "\n",
    "\n",
    "def noise_mild(x):\n",
    "    return clamp_01(x + torch.randn_like(x) * 0.025)\n",
    "\n",
    "\n",
    "def dark_mild(x):\n",
    "    return clamp_01(x * 0.65)\n",
    "\n",
    "\n",
    "def overexposed_mild(x):\n",
    "    return clamp_01(x * 1.35)\n",
    "\n",
    "\n",
    "def dirty_mild(x):\n",
    "    return add_color_patches(x, num_patches=2, color=color_dirt, alpha_range=(0.5, 0.8), size_range=(0.03, 0.08))\n",
    "\n",
    "\n",
    "def bruised_mild(x):\n",
    "    return add_color_patches(x, num_patches=1, color=color_bruise, alpha_range=(0.4, 0.7), size_range=(0.03, 0.08))\n",
    "\n",
    "\n",
    "def occlusion_small(x):\n",
    "    return add_occlusion_patch(x, area_ratio=0.10, alpha=0.5)\n",
    "\n",
    "\n",
    "blur_medium = T.GaussianBlur(kernel_size=5, sigma=1.0)\n",
    "\n",
    "\n",
    "def scenario_A(x):\n",
    "    x = blur_medium(x)\n",
    "    x = noise_mild(x)\n",
    "    if np.random.rand() < 0.7:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def scenario_B(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = dark_mild(x)\n",
    "    else:\n",
    "        x = overexposed_mild(x)\n",
    "    x = noise_mild(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def scenario_C(x):\n",
    "    x = occlusion_small(x)\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = bruised_mild(x)\n",
    "    else:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "scenario_map = {\n",
    "    \"A\": scenario_A,\n",
    "    \"B\": scenario_B,\n",
    "    \"C\": scenario_C,\n",
    "}\n",
    "\n",
    "print(\"Augmentation scenarios defined: A (blur+noise+dirty), B (light+noise), C (occlusion+bruise)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc963dae",
   "metadata": {},
   "source": [
    "## 7. Apply Augmentation to Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef4e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set augmented:\n",
      "  Total augmented: 18248 (20.0%)\n",
      "  Scenario distribution: {'A': 7293, 'B': 7358, 'C': 3597}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_augment = int(len(X_train) * AUG_RATIO)\n",
    "aug_indices = np.random.choice(len(X_train), n_augment, replace=False)\n",
    "\n",
    "scenario_counts = {\"A\": 0, \"B\": 0, \"C\": 0}\n",
    "\n",
    "for aug_idx in aug_indices:\n",
    "    r = np.random.rand()\n",
    "    if r < AUG_DIST[\"A\"]:\n",
    "        scenario_name = \"A\"\n",
    "    elif r < AUG_DIST[\"A\"] + AUG_DIST[\"B\"]:\n",
    "        scenario_name = \"B\"\n",
    "    else:\n",
    "        scenario_name = \"C\"\n",
    "    scenario_counts[scenario_name] += 1\n",
    "    img_tensor = torch.from_numpy(X_train[aug_idx]).float()\n",
    "    aug_img = scenario_map[scenario_name](img_tensor)\n",
    "    X_train[aug_idx] = aug_img.numpy()\n",
    "\n",
    "print(f\"\\nTraining set augmented:\")\n",
    "print(f\"  Total augmented: {n_augment} ({AUG_RATIO*100}%)\")\n",
    "print(f\"  Scenario distribution: {scenario_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e61a1a8",
   "metadata": {},
   "source": [
    "## 8. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8292fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined.\n"
     ]
    }
   ],
   "source": [
    "def color_hist_features(X, bins=HIST_BINS, img_shape=(3, SIZE, SIZE)):\n",
    "    \"\"\"Extract HSV color histogram features.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    feats = np.zeros((n_samples, 3 * bins), dtype=np.float32)\n",
    "    bin_edges = np.linspace(0.0, 1.0, bins + 1)\n",
    "    for i in range(n_samples):\n",
    "        img = X[i].reshape(img_shape)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "        img_hsv = (img * 255.0).astype(np.uint8)\n",
    "        img_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(img_hsv)\n",
    "        hists = []\n",
    "        for channel in (h, s, v):\n",
    "            ch_norm = channel.astype(np.float32) / 255.0\n",
    "            hist, _ = np.histogram(ch_norm.ravel(), bins=bin_edges, density=True)\n",
    "            hists.append(hist)\n",
    "        feats[i] = np.concatenate(hists)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def _prepare_img(Xi, img_shape):\n",
    "    img = Xi.reshape(img_shape)\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    return np.clip(img, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def _to_gray(img):\n",
    "    return rgb2gray(img)\n",
    "\n",
    "\n",
    "def hog_features(img_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    \"\"\"Extract HOG features.\"\"\"\n",
    "    return hog(\n",
    "        img_gray,\n",
    "        orientations=orientations,\n",
    "        pixels_per_cell=pixels_per_cell,\n",
    "        cells_per_block=cells_per_block,\n",
    "        block_norm=\"L2-Hys\",\n",
    "        transform_sqrt=True,\n",
    "        feature_vector=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def lbp_features(img_gray, P=8, R=1):\n",
    "    \"\"\"Extract LBP features.\"\"\"\n",
    "    img_u8 = np.clip(img_gray * 255.0, 0, 255).astype(np.uint8)\n",
    "    lbp = local_binary_pattern(img_u8, P=P, R=R, method=\"uniform\")\n",
    "    n_bins = P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_bins + 1), density=True)\n",
    "    return hist\n",
    "\n",
    "\n",
    "def glcm_features(img_gray, distances=(1, 2), angles=(0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)):\n",
    "    \"\"\"Extract GLCM features.\"\"\"\n",
    "    img_u8 = np.clip(img_gray * 255.0, 0, 255).astype(np.uint8)\n",
    "    glcm = graycomatrix(\n",
    "        img_u8,\n",
    "        distances=distances,\n",
    "        angles=angles,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True,\n",
    "    )\n",
    "    props = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\", \"ASM\"]\n",
    "    feats = [graycoprops(glcm, p).ravel() for p in props]\n",
    "    return np.concatenate(feats)\n",
    "\n",
    "\n",
    "def gabor_features(img_gray, frequencies=(0.1, 0.2, 0.3), thetas=(0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)):\n",
    "    \"\"\"Extract Gabor features.\"\"\"\n",
    "    feats = []\n",
    "    for freq in frequencies:\n",
    "        for theta in thetas:\n",
    "            real, imag = gabor(img_gray, frequency=freq, theta=theta)\n",
    "            mag = np.sqrt(real ** 2 + imag ** 2)\n",
    "            feats.append(mag.mean())\n",
    "            feats.append(mag.var())\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "\n",
    "def compute_feature_blocks(\n",
    "    X,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=None,\n",
    "    lbp_params=None,\n",
    "    glcm_params=None,\n",
    "    gabor_params=None,\n",
    "    feature_keys=None,\n",
    "):\n",
    "    \"\"\"Compute all requested feature blocks.\"\"\"\n",
    "    hog_params = hog_params or {}\n",
    "    lbp_params = lbp_params or {}\n",
    "    glcm_params = glcm_params or {}\n",
    "    gabor_params = gabor_params or {}\n",
    "\n",
    "    if feature_keys is None:\n",
    "        feature_keys = {\"color_hist\", \"hog\", \"lbp\", \"glcm\", \"gabor\"}\n",
    "    else:\n",
    "        feature_keys = set(feature_keys)\n",
    "\n",
    "    blocks = {}\n",
    "    if \"color_hist\" in feature_keys:\n",
    "        blocks[\"color_hist\"] = color_hist_features(X, bins=color_bins, img_shape=img_shape)\n",
    "\n",
    "    need_gray = any(k in feature_keys for k in (\"hog\", \"lbp\", \"glcm\", \"gabor\"))\n",
    "    if need_gray:\n",
    "        hog_list, lbp_list, glcm_list, gabor_list = [], [], [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            img = _prepare_img(X[i], img_shape)\n",
    "            gray = _to_gray(img)\n",
    "            if \"hog\" in feature_keys:\n",
    "                hog_list.append(hog_features(gray, **hog_params))\n",
    "            if \"lbp\" in feature_keys:\n",
    "                lbp_list.append(lbp_features(gray, **lbp_params))\n",
    "            if \"glcm\" in feature_keys:\n",
    "                glcm_list.append(glcm_features(gray, **glcm_params))\n",
    "            if \"gabor\" in feature_keys:\n",
    "                gabor_list.append(gabor_features(gray, **gabor_params))\n",
    "        if \"hog\" in feature_keys:\n",
    "            blocks[\"hog\"] = np.vstack(hog_list).astype(np.float32)\n",
    "        if \"lbp\" in feature_keys:\n",
    "            blocks[\"lbp\"] = np.vstack(lbp_list).astype(np.float32)\n",
    "        if \"glcm\" in feature_keys:\n",
    "            blocks[\"glcm\"] = np.vstack(glcm_list).astype(np.float32)\n",
    "        if \"gabor\" in feature_keys:\n",
    "            blocks[\"gabor\"] = np.vstack(gabor_list).astype(np.float32)\n",
    "    return blocks\n",
    "\n",
    "\n",
    "def concat_feature_blocks(blocks, keys):\n",
    "    \"\"\"Concatenate selected feature blocks.\"\"\"\n",
    "    return np.concatenate([blocks[k] for k in keys], axis=1)\n",
    "\n",
    "\n",
    "print(\"Feature extraction functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bcafa9",
   "metadata": {},
   "source": [
    "## 9. Degraded Test Set Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f44720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degraded test evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_fns,\n",
    "    probs,\n",
    "    feature_keys,\n",
    "    hog_params,\n",
    "    lbp_params,\n",
    "    glcm_params,\n",
    "    gabor_params,\n",
    "    scaler,\n",
    "    pca_model,\n",
    "    clf,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Evaluate model on test set with on-the-fly degradation.\"\"\"\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "    scenario_names = list(scenario_fns.keys())\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    scenario_counts = {name: 0 for name in scenario_names}\n",
    "    start = time.time()\n",
    "\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs_batch = []\n",
    "        for img in imgs:\n",
    "            r = np.random.rand()\n",
    "            # Assign scenario based on cumulative probability\n",
    "            if r < probs[0]:\n",
    "                scenario = scenario_names[0]  # clean\n",
    "            elif r < probs[0] + probs[1]:\n",
    "                scenario = scenario_names[1]  # A\n",
    "            elif r < probs[0] + probs[1] + probs[2]:\n",
    "                scenario = scenario_names[2]  # B\n",
    "            else:\n",
    "                scenario = scenario_names[3]  # C\n",
    "\n",
    "            scenario_counts[scenario] += 1\n",
    "            x = scenario_fns[scenario](img)\n",
    "            imgs_batch.append(x.unsqueeze(0))\n",
    "\n",
    "        imgs_batch = torch.cat(imgs_batch, dim=0)\n",
    "        X = imgs_batch.numpy()\n",
    "\n",
    "        # Extract features\n",
    "        blocks = compute_feature_blocks(\n",
    "            X,\n",
    "            img_shape=(3, SIZE, SIZE),\n",
    "            color_bins=HIST_BINS,\n",
    "            hog_params=hog_params,\n",
    "            lbp_params=lbp_params,\n",
    "            glcm_params=glcm_params,\n",
    "            gabor_params=gabor_params,\n",
    "            feature_keys=feature_keys,\n",
    "        )\n",
    "        X_feat = concat_feature_blocks(blocks, feature_keys)\n",
    "        X_sc = scaler.transform(X_feat)\n",
    "        X_pca = pca_model.transform(X_sc)\n",
    "        preds = clf.predict(X_pca)\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    acc = (all_preds == all_labels).mean()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"  Test accuracy (degraded): {acc:.4f}\")\n",
    "        print(f\"  Evaluation time: {elapsed:.2f}s\")\n",
    "        print(f\"  Scenario breakdown: {scenario_counts}\")\n",
    "\n",
    "    return acc, elapsed, scenario_counts\n",
    "\n",
    "\n",
    "print(\"Degraded test evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0968e5",
   "metadata": {},
   "source": [
    "## 10. Feature Parameters Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd3e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature parameters defined:\n",
      "  HOG: {'pixels_per_cell': (8, 8), 'cells_per_block': (2, 2), 'orientations': 9}\n",
      "  LBP: {'P': 8, 'R': 1}\n",
      "  GLCM: {'distances': (1, 2), 'angles': (0, 0.7853981633974483, 1.5707963267948966, 2.356194490192345)}\n",
      "  GABOR: {'frequencies': (0.1, 0.2, 0.3), 'thetas': (0, 0.7853981633974483, 1.5707963267948966, 2.356194490192345)}\n",
      "\n",
      "Feature combinations: ['Color Histogram', 'CH + HOG', 'CH + LBP', 'CH + GABOR', 'CH + GLCM', 'CH + HOG + LBP']\n"
     ]
    }
   ],
   "source": [
    "# Define optimal parameters for each descriptor\n",
    "HOG_PARAMS = {\"pixels_per_cell\": (8, 8), \"cells_per_block\": (2, 2), \"orientations\": 9}\n",
    "LBP_PARAMS = {\"P\": 8, \"R\": 1}  # P=neighbors, R=radius\n",
    "GLCM_PARAMS = {\"distances\": (1, 2), \"angles\": (0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)}\n",
    "GABOR_PARAMS = {\"frequencies\": (0.1, 0.2, 0.3), \"thetas\": (0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)}\n",
    "\n",
    "# Feature combinations\n",
    "feature_combinations = {\n",
    "    \"Color Histogram\": [\"color_hist\"],\n",
    "    \"CH + HOG\": [\"color_hist\", \"hog\"],\n",
    "    \"CH + LBP\": [\"color_hist\", \"lbp\"],\n",
    "    \"CH + GABOR\": [\"color_hist\", \"gabor\"],\n",
    "    \"CH + GLCM\": [\"color_hist\", \"glcm\"],\n",
    "    \"CH + HOG + LBP\": [\"color_hist\", \"hog\", \"lbp\"],\n",
    "}\n",
    "\n",
    "# Scenario functions for on-the-fly test degradation\n",
    "scenario_functions = {\n",
    "    \"clean\": lambda x: x,\n",
    "    \"scenario_A\": scenario_A,\n",
    "    \"scenario_B\": scenario_B,\n",
    "    \"scenario_C\": scenario_C,\n",
    "}\n",
    "\n",
    "print(\"Feature parameters defined:\")\n",
    "print(f\"  HOG: {HOG_PARAMS}\")\n",
    "print(f\"  LBP: {LBP_PARAMS}\")\n",
    "print(f\"  GLCM: {GLCM_PARAMS}\")\n",
    "print(f\"  GABOR: {GABOR_PARAMS}\")\n",
    "print(f\"\\nFeature combinations: {list(feature_combinations.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b368af4",
   "metadata": {},
   "source": [
    "## 11. Pipeline: Feature Combination 1 - Color Histogram Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a35955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 1: Color Histogram (32 bins)\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 96)\n",
      "  Val features: (39104, 96)\n",
      "  Test features: (43442, 96)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 43}\n",
      "Best CV score on subset: 0.9654\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9703\n",
      "  Evaluation time: 163.06s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/Color_Histogram\n",
      "\n",
      "Total execution time: 454.86s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 1: Color Histogram (32 bins)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"Color Histogram\"]\n",
    "feature_name = \"Color_Histogram\"\n",
    "start_total = time.time()\n",
    "\n",
    "# Extract features\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(X_train, feature_keys=feature_keys, color_bins=HIST_BINS)\n",
    "blocks_val = compute_feature_blocks(X_val, feature_keys=feature_keys, color_bins=HIST_BINS)\n",
    "blocks_test = compute_feature_blocks(X_test, feature_keys=feature_keys, color_bins=HIST_BINS)\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "# Scaling\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Grid search on subset\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_ch = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_ch = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_ch}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Fit final PCA with best parameters\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_ch = PCA(n_components=best_params_ch[\"n_components\"])\n",
    "X_train_pca = pca_ch.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_ch.transform(X_val_sc)\n",
    "X_test_pca = pca_ch.transform(X_test_sc)\n",
    "\n",
    "# Train final SVM\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_ch = SVC(kernel=\"rbf\", C=best_params_ch[\"C\"], gamma=best_params_ch[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_ch.fit(X_train_pca, y_train)\n",
    "\n",
    "# Validation accuracy\n",
    "val_acc_ch = clf_ch.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_ch:.4f}\")\n",
    "\n",
    "# Test accuracy on degraded set\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_ch, test_time_ch, scenario_counts_ch = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_ch,\n",
    "    clf_ch,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_ch, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_ch, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_ch, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "# Store results\n",
    "elapsed_total = time.time() - start_total\n",
    "result_ch = {\n",
    "    \"Feature Combination\": \"Color Histogram\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_ch[\"n_components\"],\n",
    "    \"Variance Target\": best_params_ch[\"variance_target\"],\n",
    "    \"C\": best_params_ch[\"C\"],\n",
    "    \"Gamma\": best_params_ch[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_ch:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_ch:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bed4f",
   "metadata": {},
   "source": [
    "## 12. Pipeline: Feature Combination 2 - CH + HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188ffd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 2: Color Histogram + HOG\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 420)\n",
      "  Val features: (39104, 420)\n",
      "  Test features: (43442, 420)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.001, 'n_components': 152}\n",
      "Best CV score on subset: 0.9685\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9611\n",
      "  Evaluation time: 361.71s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/CH_HOG\n",
      "\n",
      "Total execution time: 1304.77s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 2: Color Histogram + HOG\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"CH + HOG\"]\n",
    "feature_name = \"CH_HOG\"\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_hog = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_hog = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_hog}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_hog = PCA(n_components=best_params_hog[\"n_components\"])\n",
    "X_train_pca = pca_hog.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_hog.transform(X_val_sc)\n",
    "X_test_pca = pca_hog.transform(X_test_sc)\n",
    "\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_hog = SVC(kernel=\"rbf\", C=best_params_hog[\"C\"], gamma=best_params_hog[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_hog.fit(X_train_pca, y_train)\n",
    "\n",
    "val_acc_hog = clf_hog.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_hog:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_hog, test_time_hog, scenario_counts_hog = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_hog,\n",
    "    clf_hog,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_hog, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_hog, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_hog, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "result_hog = {\n",
    "    \"Feature Combination\": \"CH + HOG\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_hog[\"n_components\"],\n",
    "    \"Variance Target\": best_params_hog[\"variance_target\"],\n",
    "    \"C\": best_params_hog[\"C\"],\n",
    "    \"Gamma\": best_params_hog[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_hog:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_hog:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb0b36b",
   "metadata": {},
   "source": [
    "## 13. Pipeline: Feature Combination 3 - CH + LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8148a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 3: Color Histogram + LBP\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 106)\n",
      "  Val features: (39104, 106)\n",
      "  Test features: (43442, 106)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 47}\n",
      "Best CV score on subset: 0.9653\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9719\n",
      "  Evaluation time: 186.88s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/CH_LBP\n",
      "\n",
      "Total execution time: 522.88s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 3: Color Histogram + LBP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"CH + LBP\"]\n",
    "feature_name = \"CH_LBP\"\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_lbp = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_lbp = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_lbp}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_lbp = PCA(n_components=best_params_lbp[\"n_components\"])\n",
    "X_train_pca = pca_lbp.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_lbp.transform(X_val_sc)\n",
    "X_test_pca = pca_lbp.transform(X_test_sc)\n",
    "\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_lbp = SVC(kernel=\"rbf\", C=best_params_lbp[\"C\"], gamma=best_params_lbp[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_lbp.fit(X_train_pca, y_train)\n",
    "\n",
    "val_acc_lbp = clf_lbp.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_lbp:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_lbp, test_time_lbp, scenario_counts_lbp = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_lbp,\n",
    "    clf_lbp,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_lbp, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_lbp, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_lbp, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "result_lbp = {\n",
    "    \"Feature Combination\": \"CH + LBP\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_lbp[\"n_components\"],\n",
    "    \"Variance Target\": best_params_lbp[\"variance_target\"],\n",
    "    \"C\": best_params_lbp[\"C\"],\n",
    "    \"Gamma\": best_params_lbp[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_lbp:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_lbp:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe05e5",
   "metadata": {},
   "source": [
    "## 14. Pipeline: Feature Combination 4 - CH + GABOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de31dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 4: Color Histogram + GABOR\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 120)\n",
      "  Val features: (39104, 120)\n",
      "  Test features: (43442, 120)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 50}\n",
      "Best CV score on subset: 0.9708\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 1.0000\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9795\n",
      "  Evaluation time: 2783.37s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/CH_GABOR\n",
      "\n",
      "Total execution time: 12824.14s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 4: Color Histogram + GABOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"CH + GABOR\"]\n",
    "feature_name = \"CH_GABOR\"\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_gabor = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_gabor = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_gabor}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_gabor = PCA(n_components=best_params_gabor[\"n_components\"])\n",
    "X_train_pca = pca_gabor.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_gabor.transform(X_val_sc)\n",
    "X_test_pca = pca_gabor.transform(X_test_sc)\n",
    "\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_gabor = SVC(kernel=\"rbf\", C=best_params_gabor[\"C\"], gamma=best_params_gabor[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_gabor.fit(X_train_pca, y_train)\n",
    "\n",
    "val_acc_gabor = clf_gabor.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_gabor:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_gabor, test_time_gabor, scenario_counts_gabor = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_gabor,\n",
    "    clf_gabor,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_gabor, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_gabor, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_gabor, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "result_gabor = {\n",
    "    \"Feature Combination\": \"CH + GABOR\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_gabor[\"n_components\"],\n",
    "    \"Variance Target\": best_params_gabor[\"variance_target\"],\n",
    "    \"C\": best_params_gabor[\"C\"],\n",
    "    \"Gamma\": best_params_gabor[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_gabor:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_gabor:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7475c37",
   "metadata": {},
   "source": [
    "## 15. Pipeline: Feature Combination 5 - CH + GLCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2da1f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 5: Color Histogram + GLCM\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 144)\n",
      "  Val features: (39104, 144)\n",
      "  Test features: (43442, 144)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 43}\n",
      "Best CV score on subset: 0.9640\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 1.0000\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9742\n",
      "  Evaluation time: 1269.00s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/CH_GLCM\n",
      "\n",
      "Total execution time: 5721.52s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 5: Color Histogram + GLCM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"CH + GLCM\"]\n",
    "feature_name = \"CH_GLCM\"\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_glcm = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_glcm = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_glcm}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_glcm = PCA(n_components=best_params_glcm[\"n_components\"])\n",
    "X_train_pca = pca_glcm.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_glcm.transform(X_val_sc)\n",
    "X_test_pca = pca_glcm.transform(X_test_sc)\n",
    "\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_glcm = SVC(kernel=\"rbf\", C=best_params_glcm[\"C\"], gamma=best_params_glcm[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_glcm.fit(X_train_pca, y_train)\n",
    "\n",
    "val_acc_glcm = clf_glcm.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_glcm:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_glcm, test_time_glcm, scenario_counts_glcm = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_glcm,\n",
    "    clf_glcm,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_glcm, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_glcm, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_glcm, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "result_glcm = {\n",
    "    \"Feature Combination\": \"CH + GLCM\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_glcm[\"n_components\"],\n",
    "    \"Variance Target\": best_params_glcm[\"variance_target\"],\n",
    "    \"C\": best_params_glcm[\"C\"],\n",
    "    \"Gamma\": best_params_glcm[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_glcm:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_glcm:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959933d",
   "metadata": {},
   "source": [
    "## 16. Pipeline: Feature Combination 6 - CH + HOG + LBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b36441f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE COMBINATION 6: Color Histogram + HOG + LBP\n",
      "================================================================================\n",
      "\n",
      "Extracting features...\n",
      "  Train features: (91240, 430)\n",
      "  Val features: (39104, 430)\n",
      "  Test features: (43442, 430)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Running grid search with cross-validation...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.001, 'n_components': 155}\n",
      "Best CV score on subset: 0.9683\n",
      "\n",
      "Fitting final PCA...\n",
      "\n",
      "Training final SVM...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on degraded test set...\n",
      "  Test accuracy (degraded): 0.9621\n",
      "  Evaluation time: 474.57s\n",
      "  Scenario breakdown: {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Saving model...\n",
      "Model saved to: saved_models/hcf_degraded_comparison/CH_HOG_LBP\n",
      "\n",
      "Total execution time: 1616.60s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COMBINATION 6: Color Histogram + HOG + LBP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "feature_keys = feature_combinations[\"CH + HOG + LBP\"]\n",
    "feature_name = \"CH_HOG_LBP\"\n",
    "start_total = time.time()\n",
    "\n",
    "print(\"\\nExtracting features...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    feature_keys=feature_keys,\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    ")\n",
    "\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "\n",
    "print(f\"  Train features: {X_train_feat.shape}\")\n",
    "print(f\"  Val features: {X_val_feat.shape}\")\n",
    "print(f\"  Test features: {X_test_feat.shape}\")\n",
    "\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "print(\"\\nRunning grid search with cross-validation...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params_hog_lbp = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "\n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "\n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel=\"rbf\", C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring=\"accuracy\")\n",
    "            mean_score = scores.mean()\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params_hog_lbp = {\"variance_target\": target_var, \"C\": C, \"gamma\": gamma, \"n_components\": n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params_hog_lbp}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\nFitting final PCA...\")\n",
    "pca_hog_lbp = PCA(n_components=best_params_hog_lbp[\"n_components\"])\n",
    "X_train_pca = pca_hog_lbp.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_hog_lbp.transform(X_val_sc)\n",
    "X_test_pca = pca_hog_lbp.transform(X_test_sc)\n",
    "\n",
    "print(\"\\nTraining final SVM...\")\n",
    "clf_hog_lbp = SVC(kernel=\"rbf\", C=best_params_hog_lbp[\"C\"], gamma=best_params_hog_lbp[\"gamma\"], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_hog_lbp.fit(X_train_pca, y_train)\n",
    "\n",
    "val_acc_hog_lbp = clf_hog_lbp.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc_hog_lbp:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating on degraded test set...\")\n",
    "test_acc_hog_lbp, test_time_hog_lbp, scenario_counts_hog_lbp = evaluate_on_degraded_test(\n",
    "    test_loader,\n",
    "    scenario_functions,\n",
    "    TEST_DEGRADATION_DISTRIBUTION,\n",
    "    feature_keys,\n",
    "    HOG_PARAMS,\n",
    "    LBP_PARAMS,\n",
    "    GLCM_PARAMS,\n",
    "    GABOR_PARAMS,\n",
    "    scaler,\n",
    "    pca_hog_lbp,\n",
    "    clf_hog_lbp,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "model_dir = MODEL_SAVE_DIR / feature_name\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(clf_hog_lbp, model_dir / \"model.joblib\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n",
    "joblib.dump(pca_hog_lbp, model_dir / \"pca.joblib\")\n",
    "joblib.dump(best_params_hog_lbp, model_dir / \"params.joblib\")\n",
    "print(f\"Model saved to: {model_dir}\")\n",
    "\n",
    "elapsed_total = time.time() - start_total\n",
    "result_hog_lbp = {\n",
    "    \"Feature Combination\": \"CH + HOG + LBP\",\n",
    "    \"Feature Dimension\": X_train_feat.shape[1],\n",
    "    \"PCA Components\": best_params_hog_lbp[\"n_components\"],\n",
    "    \"Variance Target\": best_params_hog_lbp[\"variance_target\"],\n",
    "    \"C\": best_params_hog_lbp[\"C\"],\n",
    "    \"Gamma\": best_params_hog_lbp[\"gamma\"],\n",
    "    \"CV Score\": f\"{best_score:.4f}\",\n",
    "    \"Val Accuracy\": f\"{val_acc_hog_lbp:.4f}\",\n",
    "    \"Test Accuracy (Degraded)\": f\"{test_acc_hog_lbp:.4f}\",\n",
    "    \"Total Time (s)\": f\"{elapsed_total:.2f}\",\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e651b00",
   "metadata": {},
   "source": [
    "## 17. Final Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc1279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "  Feature Combination  Feature Dimension  PCA Components  Variance Target    C  Gamma CV Score Val Accuracy  Test Accuracy (Degraded) Total Time (s)\n",
      "0          CH + GABOR                120              50             0.95  100  0.010   0.9708       1.0000                    0.9795       12824.14\n",
      "1           CH + GLCM                144              43             0.95  100  0.010   0.9640       1.0000                    0.9742        5721.52\n",
      "2            CH + LBP                106              47             0.95  100  0.010   0.9653       0.9999                    0.9719         522.88\n",
      "3     Color Histogram                 96              43             0.95  100  0.010   0.9654       0.9999                    0.9703         454.86\n",
      "4      CH + HOG + LBP                430             155             0.95  100  0.001   0.9683       0.9999                    0.9621        1616.60\n",
      "5            CH + HOG                420             152             0.95  100  0.001   0.9685       0.9999                    0.9611        1304.77\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SUMMARY STATISTICS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best performing: CH + GABOR\n",
      "  Test Accuracy: 0.9795\n",
      "  Validation Accuracy: 1.0000\n",
      "  PCA Components: 50\n",
      "  SVM C=100, gamma=0.01\n",
      "\n",
      "Ranking by Test Accuracy (Degraded Test Set):\n",
      "  1. CH + GABOR           - Test: 0.9795, Val: 1.0000\n",
      "  2. CH + GLCM            - Test: 0.9742, Val: 1.0000\n",
      "  3. CH + LBP             - Test: 0.9719, Val: 0.9999\n",
      "  4. Color Histogram      - Test: 0.9703, Val: 0.9999\n",
      "  5. CH + HOG + LBP       - Test: 0.9621, Val: 0.9999\n",
      "  6. CH + HOG             - Test: 0.9611, Val: 0.9999\n",
      "\n",
      "Average Test Accuracy: 0.9699\n",
      "Average Validation Accuracy: 0.9999\n",
      "Total Execution Time: 22444.77s\n",
      "\n",
      "====================================================================================================\n",
      "All models saved to: saved_models/hcf_degraded_comparison\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Compile all results\n",
    "all_results = [\n",
    "    result_ch,\n",
    "    result_hog,\n",
    "    result_lbp,\n",
    "    result_gabor,\n",
    "    result_glcm,\n",
    "    result_hog_lbp,\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by Test Accuracy (descending)\n",
    "results_df[\"Test Accuracy (Degraded)\"] = results_df[\"Test Accuracy (Degraded)\"].astype(float)\n",
    "results_df_sorted = results_df.sort_values(\"Test Accuracy (Degraded)\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + results_df_sorted.to_string(index=True))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "best_idx = results_df_sorted[\"Test Accuracy (Degraded)\"].idxmax()\n",
    "best_result = results_df_sorted.loc[best_idx]\n",
    "\n",
    "print(f\"\\nBest performing: {best_result['Feature Combination']}\")\n",
    "print(f\"  Test Accuracy: {best_result['Test Accuracy (Degraded)']}\")\n",
    "print(f\"  Validation Accuracy: {best_result['Val Accuracy']}\")\n",
    "print(f\"  PCA Components: {best_result['PCA Components']}\")\n",
    "print(f\"  SVM C={best_result['C']}, gamma={best_result['Gamma']}\")\n",
    "\n",
    "print(\"\\nRanking by Test Accuracy (Degraded Test Set):\")\n",
    "for idx, (_, row) in enumerate(results_df_sorted.iterrows(), 1):\n",
    "    print(f\"  {idx}. {row['Feature Combination']:20s} - Test: {row['Test Accuracy (Degraded)']:>6.4f}, Val: {row['Val Accuracy']:>6s}\")\n",
    "\n",
    "print(f\"\\nAverage Test Accuracy: {results_df_sorted['Test Accuracy (Degraded)'].astype(float).mean():.4f}\")\n",
    "print(f\"Average Validation Accuracy: {results_df_sorted['Val Accuracy'].astype(float).mean():.4f}\")\n",
    "print(f\"Total Execution Time: {results_df['Total Time (s)'].astype(float).sum():.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(f\"All models saved to: {MODEL_SAVE_DIR}\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f192f76",
   "metadata": {},
   "source": [
    "## 18. Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6115868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: results/unified_comparison_results.csv\n",
      "Sorted results saved to: results/unified_comparison_results_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results to CSV\n",
    "csv_path = Path(\"results/unified_comparison_results.csv\")\n",
    "csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_df_sorted.to_csv(csv_path, index=True)\n",
    "print(f\"Results saved to: {csv_path}\")\n",
    "\n",
    "# Also save the sorted version with better formatting\n",
    "results_df_sorted.to_csv(csv_path.with_stem(\"unified_comparison_results_sorted\"), index=True)\n",
    "print(f\"Sorted results saved to: {csv_path.with_stem('unified_comparison_results_sorted')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
