{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c278502",
   "metadata": {},
   "source": [
    "# HCF + PCA + SVM on Degraded Test Set\n",
    "Pipeline: color histogram + HOG/LBP/GLCM/Gabor, then PCA, then SVM. Evaluation on degraded test set (mixed scenarios)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06a06534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Image size: 32x32\n",
      "  Augmentation ratio: 20.0%\n",
      "  Augmentation dist: {'A': 0.4, 'B': 0.4, 'C': 0.2}\n",
      "  Histogram bins: 32\n",
      "  Variance targets: [0.8, 0.9, 0.95]\n",
      "  CV folds: 3\n",
      "  Tuning subset ratio: 0.2\n",
      "  C values: [10, 100]\n",
      "  Gamma values: [0.01, 0.001]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog, local_binary_pattern, graycomatrix, graycoprops\n",
    "from skimage.filters import gabor\n",
    "from skimage.color import rgb2gray\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "SIZE = 32\n",
    "VARIETY = False\n",
    "BATCH_SIZE = 100\n",
    "AUG_RATIO = 0.20\n",
    "AUG_DIST = {\"A\": 0.4, \"B\": 0.4, \"C\": 0.2}\n",
    "HIST_BINS = 32\n",
    "VARIANCE_TARGETS = [0.80, 0.90, 0.95]\n",
    "CV_FOLDS = 3\n",
    "TUNING_SUBSET_RATIO = 0.20\n",
    "C_VALUES = [10, 100]\n",
    "GAMMA_VALUES = [0.01, 0.001]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Image size: {SIZE}x{SIZE}\")\n",
    "print(f\"  Augmentation ratio: {AUG_RATIO*100}%\")\n",
    "print(f\"  Augmentation dist: {AUG_DIST}\")\n",
    "print(f\"  Histogram bins: {HIST_BINS}\")\n",
    "print(f\"  Variance targets: {VARIANCE_TARGETS}\")\n",
    "print(f\"  CV folds: {CV_FOLDS}\")\n",
    "print(f\"  Tuning subset ratio: {TUNING_SUBSET_RATIO}\")\n",
    "print(f\"  C values: {C_VALUES}\")\n",
    "print(f\"  Gamma values: {GAMMA_VALUES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcb8b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: dataset/fruit360\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"dataset/fruit360\"\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"Training\")\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"Test\")\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/fruits-360/fruits-360-100x100\"\n",
    "CLONE_DIR = \"dataset/fruits-360-100x100\"\n",
    "\n",
    "def download_dataset():\n",
    "    os.makedirs(\"dataset\", exist_ok=True)\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, CLONE_DIR], check=True)\n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Training\"), TRAIN_DIR)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Test\"), TEST_DIR)\n",
    "    shutil.rmtree(CLONE_DIR, ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    download_dataset()\n",
    "\n",
    "assert os.path.exists(TRAIN_DIR), f\"{TRAIN_DIR} not found\"\n",
    "assert os.path.exists(TEST_DIR), f\"{TEST_DIR} not found\"\n",
    "print(f\"Dataset ready: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f4497fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fruit360FolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, variety=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.variety = variety\n",
    "        self.samples = []\n",
    "        for class_name in sorted(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            label = class_name if self.variety else class_name.split()[0]\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith((\".jpg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), label))\n",
    "        unique_labels = sorted({lbl for _, lbl in self.samples})\n",
    "        self.label_to_idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {i: lbl for lbl, i in self.label_to_idx.items()}\n",
    "        print(f\"{os.path.basename(root_dir)}: {len(self.samples)} images, {len(unique_labels)} classes\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.label_to_idx[label_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20104689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 130344 images, 79 classes\n",
      "Test: 43442 images, 79 classes\n",
      "Train 91240, Val 39104, Test 43442\n",
      "Classes: 79\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((SIZE, SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_full = Fruit360FolderDataset(TRAIN_DIR, transform=transform, variety=VARIETY)\n",
    "test_dataset = Fruit360FolderDataset(TEST_DIR, transform=transform, variety=VARIETY)\n",
    "\n",
    "train_size = int(0.7 * len(train_full))\n",
    "val_size = len(train_full) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_full,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_STATE),\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "print(f\"Classes: {len(train_full.label_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51a0b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (91240, 3, 32, 32), y_train (91240,)\n",
      "X_val (39104, 3, 32, 32), y_val (39104,)\n",
      "X_test (43442, 3, 32, 32), y_test (43442,)\n"
     ]
    }
   ],
   "source": [
    "def extract_numpy(loader):\n",
    "    X_list, y_list = [], []\n",
    "    for imgs, labels in loader:\n",
    "        X_list.append(imgs.numpy())\n",
    "        y_list.append(labels.numpy())\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = extract_numpy(train_loader)\n",
    "X_val, y_val = extract_numpy(val_loader)\n",
    "X_test, y_test = extract_numpy(test_loader)\n",
    "\n",
    "print(f\"X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "print(f\"X_test {X_test.shape}, y_test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411473ec",
   "metadata": {},
   "source": [
    "## Augmentation: Scenario Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0ad3d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation functions defined\n"
     ]
    }
   ],
   "source": [
    "def clamp_01(x):\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "def add_color_patches(x, num_patches, color, alpha_range=(0.4, 0.7), size_range=(0.05, 0.15)):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    for _ in range(num_patches):\n",
    "        s = np.random.uniform(size_range[0], size_range[1])\n",
    "        patch_area = s * H * W / 4\n",
    "        r = np.random.uniform(0.5, 1.5)\n",
    "        patch_h = int(math.sqrt(patch_area / r))\n",
    "        patch_w = int(math.sqrt(patch_area * r))\n",
    "        patch_h = max(1, min(H, patch_h))\n",
    "        patch_w = max(1, min(W, patch_w))\n",
    "        top = np.random.randint(0, H - patch_h + 1)\n",
    "        left = np.random.randint(0, W - patch_w + 1)\n",
    "        bottom = top + patch_h\n",
    "        right = left + patch_w\n",
    "        alpha = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "        patch = out[:, top:bottom, left:right]\n",
    "        blended = alpha * color + (1 - alpha) * patch\n",
    "        out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "def add_occlusion_patch(x, area_ratio=0.1, color=torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1), alpha=0.5):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    patch_area = area_ratio * H * W\n",
    "    r = np.random.uniform(0.5, 1.5)\n",
    "    patch_h = int(math.sqrt(patch_area / r))\n",
    "    patch_w = int(math.sqrt(patch_area * r))\n",
    "    patch_h = max(1, min(H, patch_h))\n",
    "    patch_w = max(1, min(W, patch_w))\n",
    "    top = np.random.randint(0, H - patch_h + 1)\n",
    "    left = np.random.randint(0, W - patch_w + 1)\n",
    "    bottom = top + patch_h\n",
    "    right = left + patch_w\n",
    "    patch = out[:, top:bottom, left:right]\n",
    "    blended = alpha * color + (1 - alpha) * patch\n",
    "    out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "color_dirt = torch.tensor([0.3, 0.25, 0.2]).view(3, 1, 1)\n",
    "color_bruise = torch.tensor([0.25, 0.2, 0.15]).view(3, 1, 1)\n",
    "\n",
    "def noise_mild(x):\n",
    "    return clamp_01(x + torch.randn_like(x) * 0.025)\n",
    "\n",
    "def dark_mild(x):\n",
    "    return clamp_01(x * 0.65)\n",
    "\n",
    "def overexposed_mild(x):\n",
    "    return clamp_01(x * 1.35)\n",
    "\n",
    "def dirty_mild(x):\n",
    "    return add_color_patches(x, num_patches=2, color=color_dirt, alpha_range=(0.5, 0.8), size_range=(0.03, 0.08))\n",
    "\n",
    "def bruised_mild(x):\n",
    "    return add_color_patches(x, num_patches=1, color=color_bruise, alpha_range=(0.4, 0.7), size_range=(0.03, 0.08))\n",
    "\n",
    "def occlusion_small(x):\n",
    "    return add_occlusion_patch(x, area_ratio=0.10, alpha=0.5)\n",
    "\n",
    "blur_medium = T.GaussianBlur(kernel_size=5, sigma=1.0)\n",
    "\n",
    "def scenario_A(x):\n",
    "    x = blur_medium(x)\n",
    "    x = noise_mild(x)\n",
    "    if np.random.rand() < 0.7:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_B(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = dark_mild(x)\n",
    "    else:\n",
    "        x = overexposed_mild(x)\n",
    "    x = noise_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_C(x):\n",
    "    x = occlusion_small(x)\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = bruised_mild(x)\n",
    "    else:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "scenario_map = {\n",
    "    \"A\": scenario_A,\n",
    "    \"B\": scenario_B,\n",
    "    \"C\": scenario_C,\n",
    "}\n",
    "\n",
    "print(\"Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee21e30",
   "metadata": {},
   "source": [
    "## Data Augmentation on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e724e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented 18248 images (20.0% of training set)\n",
      "Scenario distribution: {'A': 7293, 'B': 7358, 'C': 3597}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_augment = int(len(X_train) * AUG_RATIO)\n",
    "aug_indices = np.random.choice(len(X_train), n_augment, replace=False)\n",
    "\n",
    "scenario_counts = {\"A\": 0, \"B\": 0, \"C\": 0}\n",
    "\n",
    "for aug_idx in aug_indices:\n",
    "    r = np.random.rand()\n",
    "    if r < AUG_DIST[\"A\"]:\n",
    "        scenario_name = \"A\"\n",
    "    elif r < AUG_DIST[\"A\"] + AUG_DIST[\"B\"]:\n",
    "        scenario_name = \"B\"\n",
    "    else:\n",
    "        scenario_name = \"C\"\n",
    "    scenario_counts[scenario_name] += 1\n",
    "    img_tensor = torch.from_numpy(X_train[aug_idx]).float()\n",
    "    aug_img = scenario_map[scenario_name](img_tensor)\n",
    "    X_train[aug_idx] = aug_img.numpy()\n",
    "\n",
    "print(f\"Augmented {n_augment} images ({AUG_RATIO*100}% of training set)\")\n",
    "print(f\"Scenario distribution: {scenario_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28bf5ca",
   "metadata": {},
   "source": [
    "## Feature Extraction (Color Hist + HOG/LBP/GLCM/Gabor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b3f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_hist_features(X, bins=HIST_BINS, img_shape=(3, SIZE, SIZE)):\n",
    "    n_samples = X.shape[0]\n",
    "    feats = np.zeros((n_samples, 3 * bins), dtype=np.float32)\n",
    "    bin_edges = np.linspace(0.0, 1.0, bins + 1)\n",
    "    for i in range(n_samples):\n",
    "        img = X[i].reshape(img_shape)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "        img_hsv = (img * 255.0).astype(np.uint8)\n",
    "        img_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(img_hsv)\n",
    "        hists = []\n",
    "        for channel in (h, s, v):\n",
    "            ch_norm = channel.astype(np.float32) / 255.0\n",
    "            hist, _ = np.histogram(ch_norm.ravel(), bins=bin_edges, density=True)\n",
    "            hists.append(hist)\n",
    "        feats[i] = np.concatenate(hists)\n",
    "    return feats\n",
    "\n",
    "def _prepare_img(Xi, img_shape):\n",
    "    img = Xi.reshape(img_shape)\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = np.clip(img, 0.0, 1.0)\n",
    "    return img\n",
    "\n",
    "def _to_gray(img):\n",
    "    return rgb2gray(img)\n",
    "\n",
    "def hog_features(img_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), orientations=9):\n",
    "    return hog(\n",
    "        img_gray,\n",
    "        orientations=orientations,\n",
    "        pixels_per_cell=pixels_per_cell,\n",
    "        cells_per_block=cells_per_block,\n",
    "        block_norm=\"L2-Hys\",\n",
    "        transform_sqrt=True,\n",
    "        feature_vector=True,\n",
    "    )\n",
    "\n",
    "def lbp_features(img_gray, P=8, R=1):\n",
    "    img_u8 = np.clip(img_gray * 255.0, 0, 255).astype(np.uint8)\n",
    "    lbp = local_binary_pattern(img_u8, P=P, R=R, method=\"uniform\")\n",
    "    n_bins = P + 2\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_bins + 1), density=True)\n",
    "    return hist\n",
    "\n",
    "def glcm_features(img_gray, distances=(1, 2), angles=(0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)):\n",
    "    img_u8 = np.clip(img_gray * 255.0, 0, 255).astype(np.uint8)\n",
    "    glcm = graycomatrix(\n",
    "        img_u8,\n",
    "        distances=distances,\n",
    "        angles=angles,\n",
    "        levels=256,\n",
    "        symmetric=True,\n",
    "        normed=True,\n",
    "    )\n",
    "    props = [\"contrast\", \"dissimilarity\", \"homogeneity\", \"energy\", \"correlation\", \"ASM\"]\n",
    "    feats = [graycoprops(glcm, p).ravel() for p in props]\n",
    "    return np.concatenate(feats)\n",
    "\n",
    "def gabor_features(img_gray, frequencies=(0.1, 0.2, 0.3), thetas=(0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)):\n",
    "    feats = []\n",
    "    for freq in frequencies:\n",
    "        for theta in thetas:\n",
    "            real, imag = gabor(img_gray, frequency=freq, theta=theta)\n",
    "            mag = np.sqrt(real ** 2 + imag ** 2)\n",
    "            feats.append(mag.mean())\n",
    "            feats.append(mag.var())\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def compute_feature_blocks(\n",
    "    X,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=None,\n",
    "    lbp_params=None,\n",
    "    glcm_params=None,\n",
    "    gabor_params=None,\n",
    "    feature_keys=None,\n",
    " ):\n",
    "    hog_params = hog_params or {}\n",
    "    lbp_params = lbp_params or {}\n",
    "    glcm_params = glcm_params or {}\n",
    "    gabor_params = gabor_params or {}\n",
    "    if feature_keys is None:\n",
    "        feature_keys = {\"color_hist\", \"hog\", \"lbp\", \"glcm\", \"gabor\"}\n",
    "    else:\n",
    "        feature_keys = set(feature_keys)\n",
    "\n",
    "    blocks = {}\n",
    "    if \"color_hist\" in feature_keys:\n",
    "        blocks[\"color_hist\"] = color_hist_features(X, bins=color_bins, img_shape=img_shape)\n",
    "\n",
    "    need_gray = any(k in feature_keys for k in (\"hog\", \"lbp\", \"glcm\", \"gabor\"))\n",
    "    if need_gray:\n",
    "        hog_list, lbp_list, glcm_list, gabor_list = [], [], [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            img = _prepare_img(X[i], img_shape)\n",
    "            gray = _to_gray(img)\n",
    "            if \"hog\" in feature_keys:\n",
    "                hog_list.append(hog_features(gray, **hog_params))\n",
    "            if \"lbp\" in feature_keys:\n",
    "                lbp_list.append(lbp_features(gray, **lbp_params))\n",
    "            if \"glcm\" in feature_keys:\n",
    "                glcm_list.append(glcm_features(gray, **glcm_params))\n",
    "            if \"gabor\" in feature_keys:\n",
    "                gabor_list.append(gabor_features(gray, **gabor_params))\n",
    "        if \"hog\" in feature_keys:\n",
    "            blocks[\"hog\"] = np.vstack(hog_list).astype(np.float32)\n",
    "        if \"lbp\" in feature_keys:\n",
    "            blocks[\"lbp\"] = np.vstack(lbp_list).astype(np.float32)\n",
    "        if \"glcm\" in feature_keys:\n",
    "            blocks[\"glcm\"] = np.vstack(glcm_list).astype(np.float32)\n",
    "        if \"gabor\" in feature_keys:\n",
    "            blocks[\"gabor\"] = np.vstack(gabor_list).astype(np.float32)\n",
    "    return blocks\n",
    "\n",
    "def concat_feature_blocks(blocks, keys):\n",
    "    return np.concatenate([blocks[k] for k in keys], axis=1)\n",
    "\n",
    "FEATURE_KEYS = [\"color_hist\", \"hog\", \"lbp\", \"glcm\", \"gabor\"]\n",
    "HOG_PARAMS = {\"pixels_per_cell\": (8, 8), \"cells_per_block\": (2, 2), \"orientations\": 9}\n",
    "LBP_PARAMS = {\"P\": 8, \"R\": 1}\n",
    "GLCM_PARAMS = {\"distances\": (1, 2), \"angles\": (0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)}\n",
    "GABOR_PARAMS = {\"frequencies\": (0.1, 0.2, 0.3), \"thetas\": (0, np.pi / 4, np.pi / 2, 3 * np.pi / 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b756016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"hsv+lbp\": [\"color_hist\", \"lbp\"],\n",
    "    \"hsv+glcm\": [\"color_hist\", \"glcm\"],\n",
    "    \"hsv+gabor\": [\"color_hist\", \"gabor\"],\n",
    "    \"hsv+hog\": [\"color_hist\", \"hog\"],\n",
    "    \"hsv+hog+lbp\": [\"color_hist\", \"hog\", \"lbp\"],\n",
    "}\n",
    "\n",
    "scenarios_mixed = {\n",
    "    \"clean\": lambda x: x,\n",
    "    \"scenario_A\": scenario_A,\n",
    "    \"scenario_B\": scenario_B,\n",
    "    \"scenario_C\": scenario_C,\n",
    "}\n",
    "probs_distribution = [0.60, 0.15, 0.15, 0.10]\n",
    "\n",
    "def evaluate_mixed_scenarios(test_loader, scenario_fns, probs, feature_keys, scaler, pca_model, clf, verbose=True):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    scenario_names = list(scenario_fns.keys())\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    scenario_counts = {name: 0 for name in scenario_names}\n",
    "    start = time.time()\n",
    "\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs_batch = []\n",
    "        for img in imgs:\n",
    "            r = np.random.rand()\n",
    "            if r < probs[0]:\n",
    "                scenario = scenario_names[0]\n",
    "            elif r < probs[0] + probs[1]:\n",
    "                scenario = scenario_names[1]\n",
    "            elif r < probs[0] + probs[1] + probs[2]:\n",
    "                scenario = scenario_names[2]\n",
    "            else:\n",
    "                scenario = scenario_names[3]\n",
    "            scenario_counts[scenario] += 1\n",
    "            x = scenario_fns[scenario](img)\n",
    "            imgs_batch.append(x.unsqueeze(0))\n",
    "\n",
    "        imgs_batch = torch.cat(imgs_batch, dim=0)\n",
    "        X = imgs_batch.numpy()\n",
    "        blocks = compute_feature_blocks(\n",
    "            X,\n",
    "            img_shape=(3, SIZE, SIZE),\n",
    "            color_bins=HIST_BINS,\n",
    "            hog_params=HOG_PARAMS,\n",
    "            lbp_params=LBP_PARAMS,\n",
    "            glcm_params=GLCM_PARAMS,\n",
    "            gabor_params=GABOR_PARAMS,\n",
    "            feature_keys=feature_keys,\n",
    "        )\n",
    "        X_feat = concat_feature_blocks(blocks, feature_keys)\n",
    "        X_sc = scaler.transform(X_feat)\n",
    "        X_pca = pca_model.transform(X_sc)\n",
    "        preds = clf.predict(X_pca)\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    acc = (all_preds == all_labels).mean()\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Accuracy on mixed realistic distribution: {acc:.4f}\")\n",
    "        print(f\"Time: {elapsed:.2f}s\")\n",
    "        print(f\"Scenario distribution (actual): {scenario_counts}\")\n",
    "\n",
    "    return acc, elapsed, scenario_counts\n",
    "\n",
    "results_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3487fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE COMBINATION 1: HSV + LBP\n",
      "======================================================================\n",
      "Feature keys: ['color_hist', 'lbp']\n",
      "\n",
      "Extracting features for training set...\n",
      "Train features shape: (91240, 106)\n",
      "Extracting features for validation set...\n",
      "Validation features shape: (39104, 106)\n",
      "Extracting features for test set...\n",
      "Test features shape: (43442, 106)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Starting parameter tuning on subset...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 47}\n",
      "Best CV score on subset: 0.9653\n",
      "\n",
      "Training final SVM model...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on mixed degraded test set...\n",
      "Accuracy on mixed realistic distribution: 0.9719\n",
      "Time: 200.73s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Total execution time: 737.05s\n",
      "Results stored: {'feature_combination': 'HSV+LBP', 'variance_target': 0.95, 'n_components': 47, 'C': 100, 'gamma': 0.01, 'best_cv_score': np.float64(0.9652564602468656), 'val_acc': 0.9999488543371522, 'test_acc_mixed': np.float64(0.9718935592283965), 'total_time': 737.0537121295929}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE COMBINATION 1: HSV + LBP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_keys = [\"color_hist\", \"lbp\"]\n",
    "print(f\"Feature keys: {feature_keys}\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Step 1: Extract features for train, val, test\n",
    "print(\"\\nExtracting features for training set...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for validation set...\")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "print(f\"Validation features shape: {X_val_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for test set...\")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "print(f\"Test features shape: {X_test_feat.shape}\")\n",
    "\n",
    "# Step 2: StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Step 3: Parameter tuning on subset (variance target, C, gamma)\n",
    "print(\"\\nStarting parameter tuning on subset...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    # Compute number of components for this variance target\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "    \n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'variance_target': target_var, 'C': C, 'gamma': gamma, 'n_components': n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Step 4: Fit final PCA with best n_components\n",
    "pca_final = PCA(n_components=best_params['n_components'])\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "\n",
    "# Step 5: Train final SVM with best C and gamma\n",
    "print(\"\\nTraining final SVM model...\")\n",
    "clf_final = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_final.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 6: Evaluate on validation set\n",
    "val_acc = clf_final.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Step 7: Evaluate on mixed degraded test set\n",
    "print(\"\\nEvaluating on mixed degraded test set...\")\n",
    "test_acc_mixed, test_time, scenario_counts = evaluate_mixed_scenarios(\n",
    "    test_loader, scenarios_mixed, probs_distribution, feature_keys, scaler, pca_final, clf_final, verbose=True\n",
    ")\n",
    "\n",
    "# Step 8: Compile results\n",
    "elapsed_total = time.time() - start_total\n",
    "results_dict = {\n",
    "    'feature_combination': 'HSV+LBP',\n",
    "    'variance_target': best_params['variance_target'],\n",
    "    'n_components': best_params['n_components'],\n",
    "    'C': best_params['C'],\n",
    "    'gamma': best_params['gamma'],\n",
    "    'best_cv_score': best_score,\n",
    "    'val_acc': val_acc,\n",
    "    'test_acc_mixed': test_acc_mixed,\n",
    "    'total_time': elapsed_total,\n",
    "}\n",
    "\n",
    "results_all.append(results_dict)\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")\n",
    "print(f\"Results stored: {results_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7fc21b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE COMBINATION 2: HSV + GLCM\n",
      "======================================================================\n",
      "Feature keys: ['color_hist', 'glcm']\n",
      "\n",
      "Extracting features for training set...\n",
      "Train features shape: (91240, 144)\n",
      "Extracting features for validation set...\n",
      "Validation features shape: (39104, 144)\n",
      "Extracting features for test set...\n",
      "Test features shape: (43442, 144)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Starting parameter tuning on subset...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 43}\n",
      "Best CV score on subset: 0.9640\n",
      "\n",
      "Training final SVM model...\n",
      "Validation accuracy: 1.0000\n",
      "\n",
      "Evaluating on mixed degraded test set...\n",
      "Accuracy on mixed realistic distribution: 0.9742\n",
      "Time: 1184.66s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Total execution time: 5912.42s\n",
      "Results stored: {'feature_combination': 'HSV+GLCM', 'variance_target': 0.95, 'n_components': 43, 'C': 100, 'gamma': 0.01, 'best_cv_score': np.float64(0.9639961082045839), 'val_acc': 0.9999744271685761, 'test_acc_mixed': np.float64(0.9742184982275217), 'total_time': 5912.42125582695}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE COMBINATION 2: HSV + GLCM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_keys = [\"color_hist\", \"glcm\"]\n",
    "print(f\"Feature keys: {feature_keys}\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Step 1: Extract features for train, val, test\n",
    "print(\"\\nExtracting features for training set...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for validation set...\")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "print(f\"Validation features shape: {X_val_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for test set...\")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "print(f\"Test features shape: {X_test_feat.shape}\")\n",
    "\n",
    "# Step 2: StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Step 3: Parameter tuning on subset (variance target, C, gamma)\n",
    "print(\"\\nStarting parameter tuning on subset...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    # Compute number of components for this variance target\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "    \n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'variance_target': target_var, 'C': C, 'gamma': gamma, 'n_components': n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Step 4: Fit final PCA with best n_components\n",
    "pca_final = PCA(n_components=best_params['n_components'])\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "\n",
    "# Step 5: Train final SVM with best C and gamma\n",
    "print(\"\\nTraining final SVM model...\")\n",
    "clf_final = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_final.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 6: Evaluate on validation set\n",
    "val_acc = clf_final.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Step 7: Evaluate on mixed degraded test set\n",
    "print(\"\\nEvaluating on mixed degraded test set...\")\n",
    "test_acc_mixed, test_time, scenario_counts = evaluate_mixed_scenarios(\n",
    "    test_loader, scenarios_mixed, probs_distribution, feature_keys, scaler, pca_final, clf_final, verbose=True\n",
    ")\n",
    "\n",
    "# Step 8: Compile results\n",
    "elapsed_total = time.time() - start_total\n",
    "results_dict = {\n",
    "    'feature_combination': 'HSV+GLCM',\n",
    "    'variance_target': best_params['variance_target'],\n",
    "    'n_components': best_params['n_components'],\n",
    "    'C': best_params['C'],\n",
    "    'gamma': best_params['gamma'],\n",
    "    'best_cv_score': best_score,\n",
    "    'val_acc': val_acc,\n",
    "    'test_acc_mixed': test_acc_mixed,\n",
    "    'total_time': elapsed_total,\n",
    "}\n",
    "\n",
    "results_all.append(results_dict)\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")\n",
    "print(f\"Results stored: {results_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13586a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE COMBINATION 3: HSV + GABOR\n",
      "======================================================================\n",
      "Feature keys: ['color_hist', 'gabor']\n",
      "\n",
      "Extracting features for training set...\n",
      "Train features shape: (91240, 120)\n",
      "Extracting features for validation set...\n",
      "Validation features shape: (39104, 120)\n",
      "Extracting features for test set...\n",
      "Test features shape: (43442, 120)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Starting parameter tuning on subset...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.01, 'n_components': 50}\n",
      "Best CV score on subset: 0.9708\n",
      "\n",
      "Training final SVM model...\n",
      "Validation accuracy: 1.0000\n",
      "\n",
      "Evaluating on mixed degraded test set...\n",
      "Accuracy on mixed realistic distribution: 0.9795\n",
      "Time: 2640.63s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Total execution time: 13062.78s\n",
      "Results stored: {'feature_combination': 'HSV+GABOR', 'variance_target': 0.95, 'n_components': 50, 'C': 100, 'gamma': 0.01, 'best_cv_score': np.float64(0.9708461409704755), 'val_acc': 0.9999744271685761, 'test_acc_mixed': np.float64(0.979466875374062), 'total_time': 13062.781674861908}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE COMBINATION 3: HSV + GABOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_keys = [\"color_hist\", \"gabor\"]\n",
    "print(f\"Feature keys: {feature_keys}\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Step 1: Extract features for train, val, test\n",
    "print(\"\\nExtracting features for training set...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for validation set...\")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "print(f\"Validation features shape: {X_val_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for test set...\")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "print(f\"Test features shape: {X_test_feat.shape}\")\n",
    "\n",
    "# Step 2: StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Step 3: Parameter tuning on subset (variance target, C, gamma)\n",
    "print(\"\\nStarting parameter tuning on subset...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    # Compute number of components for this variance target\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "    \n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'variance_target': target_var, 'C': C, 'gamma': gamma, 'n_components': n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Step 4: Fit final PCA with best n_components\n",
    "pca_final = PCA(n_components=best_params['n_components'])\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "\n",
    "# Step 5: Train final SVM with best C and gamma\n",
    "print(\"\\nTraining final SVM model...\")\n",
    "clf_final = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_final.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 6: Evaluate on validation set\n",
    "val_acc = clf_final.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Step 7: Evaluate on mixed degraded test set\n",
    "print(\"\\nEvaluating on mixed degraded test set...\")\n",
    "test_acc_mixed, test_time, scenario_counts = evaluate_mixed_scenarios(\n",
    "    test_loader, scenarios_mixed, probs_distribution, feature_keys, scaler, pca_final, clf_final, verbose=True\n",
    ")\n",
    "\n",
    "# Step 8: Compile results\n",
    "elapsed_total = time.time() - start_total\n",
    "results_dict = {\n",
    "    'feature_combination': 'HSV+GABOR',\n",
    "    'variance_target': best_params['variance_target'],\n",
    "    'n_components': best_params['n_components'],\n",
    "    'C': best_params['C'],\n",
    "    'gamma': best_params['gamma'],\n",
    "    'best_cv_score': best_score,\n",
    "    'val_acc': val_acc,\n",
    "    'test_acc_mixed': test_acc_mixed,\n",
    "    'total_time': elapsed_total,\n",
    "}\n",
    "\n",
    "results_all.append(results_dict)\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")\n",
    "print(f\"Results stored: {results_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3be60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE COMBINATION 4: HSV + HOG\n",
      "======================================================================\n",
      "Feature keys: ['color_hist', 'hog']\n",
      "\n",
      "Extracting features for training set...\n",
      "Train features shape: (91240, 420)\n",
      "Extracting features for validation set...\n",
      "Validation features shape: (39104, 420)\n",
      "Extracting features for test set...\n",
      "Test features shape: (43442, 420)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Starting parameter tuning on subset...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.001, 'n_components': 152}\n",
      "Best CV score on subset: 0.9685\n",
      "\n",
      "Training final SVM model...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on mixed degraded test set...\n",
      "Accuracy on mixed realistic distribution: 0.9611\n",
      "Time: 429.65s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Total execution time: 2147.26s\n",
      "Results stored: {'feature_combination': 'HSV+HOG', 'variance_target': 0.95, 'n_components': 152, 'C': 100, 'gamma': 0.001, 'best_cv_score': np.float64(0.968544464802358), 'val_acc': 0.9998977086743044, 'test_acc_mixed': np.float64(0.9610745361631601), 'total_time': 2147.2617321014404}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE COMBINATION 4: HSV + HOG\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_keys = [\"color_hist\", \"hog\"]\n",
    "print(f\"Feature keys: {feature_keys}\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Step 1: Extract features for train, val, test\n",
    "print(\"\\nExtracting features for training set...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for validation set...\")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "print(f\"Validation features shape: {X_val_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for test set...\")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "print(f\"Test features shape: {X_test_feat.shape}\")\n",
    "\n",
    "# Step 2: StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Step 3: Parameter tuning on subset (variance target, C, gamma)\n",
    "print(\"\\nStarting parameter tuning on subset...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    # Compute number of components for this variance target\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "    \n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'variance_target': target_var, 'C': C, 'gamma': gamma, 'n_components': n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Step 4: Fit final PCA with best n_components\n",
    "pca_final = PCA(n_components=best_params['n_components'])\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "\n",
    "# Step 5: Train final SVM with best C and gamma\n",
    "print(\"\\nTraining final SVM model...\")\n",
    "clf_final = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_final.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 6: Evaluate on validation set\n",
    "val_acc = clf_final.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Step 7: Evaluate on mixed degraded test set\n",
    "print(\"\\nEvaluating on mixed degraded test set...\")\n",
    "test_acc_mixed, test_time, scenario_counts = evaluate_mixed_scenarios(\n",
    "    test_loader, scenarios_mixed, probs_distribution, feature_keys, scaler, pca_final, clf_final, verbose=True\n",
    ")\n",
    "\n",
    "# Step 8: Compile results\n",
    "elapsed_total = time.time() - start_total\n",
    "results_dict = {\n",
    "    'feature_combination': 'HSV+HOG',\n",
    "    'variance_target': best_params['variance_target'],\n",
    "    'n_components': best_params['n_components'],\n",
    "    'C': best_params['C'],\n",
    "    'gamma': best_params['gamma'],\n",
    "    'best_cv_score': best_score,\n",
    "    'val_acc': val_acc,\n",
    "    'test_acc_mixed': test_acc_mixed,\n",
    "    'total_time': elapsed_total,\n",
    "}\n",
    "\n",
    "results_all.append(results_dict)\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")\n",
    "print(f\"Results stored: {results_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a22109f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE COMBINATION 5: HSV + HOG + LBP\n",
      "======================================================================\n",
      "Feature keys: ['color_hist', 'hog', 'lbp']\n",
      "\n",
      "Extracting features for training set...\n",
      "Train features shape: (91240, 430)\n",
      "Extracting features for validation set...\n",
      "Validation features shape: (39104, 430)\n",
      "Extracting features for test set...\n",
      "Test features shape: (43442, 430)\n",
      "\n",
      "Applying StandardScaler...\n",
      "\n",
      "Starting parameter tuning on subset...\n",
      "\n",
      "Best parameters: {'variance_target': 0.95, 'C': 100, 'gamma': 0.001, 'n_components': 155}\n",
      "Best CV score on subset: 0.9683\n",
      "\n",
      "Training final SVM model...\n",
      "Validation accuracy: 0.9999\n",
      "\n",
      "Evaluating on mixed degraded test set...\n",
      "Accuracy on mixed realistic distribution: 0.9621\n",
      "Time: 362.14s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "\n",
      "Total execution time: 2168.57s\n",
      "Results stored: {'feature_combination': 'HSV+HOG+LBP', 'variance_target': 0.95, 'n_components': 155, 'C': 100, 'gamma': 0.001, 'best_cv_score': np.float64(0.9683253467880443), 'val_acc': 0.9998977086743044, 'test_acc_mixed': np.float64(0.9620643616776392), 'total_time': 2168.5653529167175}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE COMBINATION 5: HSV + HOG + LBP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "feature_keys = [\"color_hist\", \"hog\", \"lbp\"]\n",
    "print(f\"Feature keys: {feature_keys}\")\n",
    "\n",
    "start_total = time.time()\n",
    "\n",
    "# Step 1: Extract features for train, val, test\n",
    "print(\"\\nExtracting features for training set...\")\n",
    "blocks_train = compute_feature_blocks(\n",
    "    X_train,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_train_feat = concat_feature_blocks(blocks_train, feature_keys)\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for validation set...\")\n",
    "blocks_val = compute_feature_blocks(\n",
    "    X_val,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_val_feat = concat_feature_blocks(blocks_val, feature_keys)\n",
    "print(f\"Validation features shape: {X_val_feat.shape}\")\n",
    "\n",
    "print(\"Extracting features for test set...\")\n",
    "blocks_test = compute_feature_blocks(\n",
    "    X_test,\n",
    "    img_shape=(3, SIZE, SIZE),\n",
    "    color_bins=HIST_BINS,\n",
    "    hog_params=HOG_PARAMS,\n",
    "    lbp_params=LBP_PARAMS,\n",
    "    glcm_params=GLCM_PARAMS,\n",
    "    gabor_params=GABOR_PARAMS,\n",
    "    feature_keys=feature_keys,\n",
    ")\n",
    "X_test_feat = concat_feature_blocks(blocks_test, feature_keys)\n",
    "print(f\"Test features shape: {X_test_feat.shape}\")\n",
    "\n",
    "# Step 2: StandardScaler\n",
    "print(\"\\nApplying StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_feat)\n",
    "X_val_sc = scaler.transform(X_val_feat)\n",
    "X_test_sc = scaler.transform(X_test_feat)\n",
    "\n",
    "# Step 3: Parameter tuning on subset (variance target, C, gamma)\n",
    "print(\"\\nStarting parameter tuning on subset...\")\n",
    "tuning_subset_size = int(len(X_train_sc) * TUNING_SUBSET_RATIO)\n",
    "X_train_subset = X_train_sc[:tuning_subset_size]\n",
    "y_train_subset = y_train[:tuning_subset_size]\n",
    "\n",
    "best_score = -np.inf\n",
    "best_params = {}\n",
    "\n",
    "for target_var in VARIANCE_TARGETS:\n",
    "    # Compute number of components for this variance target\n",
    "    pca_temp = PCA()\n",
    "    pca_temp.fit(X_train_subset)\n",
    "    cumsum = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "    n_comp = int(np.argmax(cumsum >= target_var) + 1)\n",
    "    n_comp = max(1, min(n_comp, len(cumsum)))\n",
    "    \n",
    "    X_train_subset_pca = pca_temp.transform(X_train_subset)[:, :n_comp]\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            clf = SVC(kernel='rbf', C=C, gamma=gamma, random_state=RANDOM_STATE, verbose=0)\n",
    "            scores = cross_val_score(clf, X_train_subset_pca, y_train_subset, cv=CV_FOLDS, scoring='accuracy')\n",
    "            mean_score = scores.mean()\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'variance_target': target_var, 'C': C, 'gamma': gamma, 'n_components': n_comp}\n",
    "\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best CV score on subset: {best_score:.4f}\")\n",
    "\n",
    "# Step 4: Fit final PCA with best n_components\n",
    "pca_final = PCA(n_components=best_params['n_components'])\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "\n",
    "# Step 5: Train final SVM with best C and gamma\n",
    "print(\"\\nTraining final SVM model...\")\n",
    "clf_final = SVC(kernel='rbf', C=best_params['C'], gamma=best_params['gamma'], random_state=RANDOM_STATE, verbose=0)\n",
    "clf_final.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 6: Evaluate on validation set\n",
    "val_acc = clf_final.score(X_val_pca, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Step 7: Evaluate on mixed degraded test set\n",
    "print(\"\\nEvaluating on mixed degraded test set...\")\n",
    "test_acc_mixed, test_time, scenario_counts = evaluate_mixed_scenarios(\n",
    "    test_loader, scenarios_mixed, probs_distribution, feature_keys, scaler, pca_final, clf_final, verbose=True\n",
    ")\n",
    "\n",
    "# Step 8: Compile results\n",
    "elapsed_total = time.time() - start_total\n",
    "results_dict = {\n",
    "    'feature_combination': 'HSV+HOG+LBP',\n",
    "    'variance_target': best_params['variance_target'],\n",
    "    'n_components': best_params['n_components'],\n",
    "    'C': best_params['C'],\n",
    "    'gamma': best_params['gamma'],\n",
    "    'best_cv_score': best_score,\n",
    "    'val_acc': val_acc,\n",
    "    'test_acc_mixed': test_acc_mixed,\n",
    "    'total_time': elapsed_total,\n",
    "}\n",
    "\n",
    "results_all.append(results_dict)\n",
    "print(f\"\\nTotal execution time: {elapsed_total:.2f}s\")\n",
    "print(f\"Results stored: {results_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64a740a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON TABLE\n",
      "======================================================================\n",
      "\n",
      "Detailed Results (sorted by Mixed Test Accuracy):\n",
      "  feature_combination  variance_target  n_components    C  gamma  best_cv_score   val_acc  test_acc_mixed    total_time\n",
      "0           HSV+GABOR             0.95            50  100  0.010       0.970846  0.999974        0.979467  13062.781675\n",
      "1            HSV+GLCM             0.95            43  100  0.010       0.963996  0.999974        0.974218   5912.421256\n",
      "2             HSV+LBP             0.95            47  100  0.010       0.965256  0.999949        0.971894    737.053712\n",
      "3         HSV+HOG+LBP             0.95           155  100  0.001       0.968325  0.999898        0.962064   2168.565353\n",
      "4             HSV+HOG             0.95           152  100  0.001       0.968544  0.999898        0.961075   2147.261732\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SUMMARY STATISTICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Best Overall Performance:\n",
      "  Feature Combination: HSV+GABOR\n",
      "  Mixed Test Accuracy: 0.9795\n",
      "  Validation Accuracy: 1.0000\n",
      "  Best CV Score: 0.9708\n",
      "  Parameters: Variance=0.95, C=100, Gamma=0.01\n",
      "  PCA Components: 50\n",
      "  Execution Time: 13062.78s\n",
      "\n",
      "Ranking by Mixed Test Accuracy:\n",
      "  1. HSV+GABOR            - Test Acc: 0.9795, Val Acc: 1.0000\n",
      "  2. HSV+GLCM             - Test Acc: 0.9742, Val Acc: 1.0000\n",
      "  3. HSV+LBP              - Test Acc: 0.9719, Val Acc: 0.9999\n",
      "  4. HSV+HOG+LBP          - Test Acc: 0.9621, Val Acc: 0.9999\n",
      "  5. HSV+HOG              - Test Acc: 0.9611, Val Acc: 0.9999\n",
      "\n",
      "Average Execution Time: 4805.62s\n",
      "Total Execution Time: 24028.08s\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create DataFrame from all results\n",
    "results_df = pd.DataFrame(results_all)\n",
    "\n",
    "# Sort by test_acc_mixed (best performance first)\n",
    "results_df_sorted = results_df.sort_values('test_acc_mixed', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display full results table\n",
    "print(\"\\nDetailed Results (sorted by Mixed Test Accuracy):\")\n",
    "print(results_df_sorted.to_string(index=True))\n",
    "\n",
    "# Create summary display\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nBest Overall Performance:\")\n",
    "best_idx = results_df_sorted['test_acc_mixed'].idxmax()\n",
    "best_result = results_df_sorted.loc[best_idx]\n",
    "print(f\"  Feature Combination: {best_result['feature_combination']}\")\n",
    "print(f\"  Mixed Test Accuracy: {best_result['test_acc_mixed']:.4f}\")\n",
    "print(f\"  Validation Accuracy: {best_result['val_acc']:.4f}\")\n",
    "print(f\"  Best CV Score: {best_result['best_cv_score']:.4f}\")\n",
    "print(f\"  Parameters: Variance={best_result['variance_target']:.2f}, C={best_result['C']}, Gamma={best_result['gamma']}\")\n",
    "print(f\"  PCA Components: {best_result['n_components']}\")\n",
    "print(f\"  Execution Time: {best_result['total_time']:.2f}s\")\n",
    "\n",
    "print(\"\\nRanking by Mixed Test Accuracy:\")\n",
    "for idx, row in results_df_sorted.iterrows():\n",
    "    print(f\"  {idx+1}. {row['feature_combination']:20s} - Test Acc: {row['test_acc_mixed']:.4f}, Val Acc: {row['val_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Execution Time: {:.2f}s\".format(results_df['total_time'].mean()))\n",
    "print(\"Total Execution Time: {:.2f}s\".format(results_df['total_time'].sum()))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
