{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5063762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3e73159e30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import cv2\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17e1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 43442 images, 79 classes\n",
      "Test loader 32x32: 43442 images, 435 batches\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"dataset/fruit360\"\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"Test\")\n",
    "CKPT_ROOT = Path(\"artifacts/checkpoints/fruit360\")\n",
    "\n",
    "SIZE = 32\n",
    "COLOR_BINS = 16\n",
    "\n",
    "class Fruit360FolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, variety=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.variety = variety\n",
    "        self.samples = []\n",
    "        \n",
    "        for class_name in sorted(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            label = class_name if self.variety else class_name.split()[0]\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.jpg', '.png')):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), label))\n",
    "        \n",
    "        unique_labels = sorted({lbl for _, lbl in self.samples})\n",
    "        self.label_to_idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {i: lbl for i, lbl in self.label_to_idx.items()}\n",
    "        \n",
    "        print(f\"{os.path.basename(root_dir)}: {len(self.samples)} images, {len(unique_labels)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label_str\n",
    "\n",
    "transform_32 = T.Compose([\n",
    "    T.Resize((SIZE, SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "test_dataset_32 = Fruit360FolderDataset(TEST_DIR, transform=transform_32, variety=False)\n",
    "test_loader_32 = DataLoader(test_dataset_32, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Test loader 32x32: {len(test_dataset_32)} images, {len(test_loader_32)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73220969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clean', 'scenario_A', 'scenario_B', 'scenario_C']\n"
     ]
    }
   ],
   "source": [
    "def clamp_01(x):\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "def add_color_patches(x, num_patches, color, alpha_range=(0.4, 0.7), size_range=(0.05, 0.15)):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    for _ in range(num_patches):\n",
    "        s = np.random.uniform(size_range[0], size_range[1])\n",
    "        patch_area = s * H * W / 4\n",
    "        r = np.random.uniform(0.5, 1.5)\n",
    "        patch_h = int(math.sqrt(patch_area / r))\n",
    "        patch_w = int(math.sqrt(patch_area * r))\n",
    "        patch_h = max(1, min(H, patch_h))\n",
    "        patch_w = max(1, min(W, patch_w))\n",
    "        top = np.random.randint(0, H - patch_h + 1)\n",
    "        left = np.random.randint(0, W - patch_w + 1)\n",
    "        bottom = top + patch_h\n",
    "        right = left + patch_w\n",
    "        alpha = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "        patch = out[:, top:bottom, left:right]\n",
    "        blended = alpha * color + (1 - alpha) * patch\n",
    "        out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "def add_occlusion_patch(x, area_ratio=0.1, color=torch.tensor([0.5, 0.5, 0.5]).view(3,1,1), alpha=0.5):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    patch_area = area_ratio * H * W\n",
    "    r = np.random.uniform(0.5, 1.5)\n",
    "    patch_h = int(math.sqrt(patch_area / r))\n",
    "    patch_w = int(math.sqrt(patch_area * r))\n",
    "    patch_h = max(1, min(H, patch_h))\n",
    "    patch_w = max(1, min(W, patch_w))\n",
    "    top = np.random.randint(0, H - patch_h + 1)\n",
    "    left = np.random.randint(0, W - patch_w + 1)\n",
    "    bottom = top + patch_h\n",
    "    right = left + patch_w\n",
    "    patch = out[:, top:bottom, left:right]\n",
    "    blended = alpha * color + (1 - alpha) * patch\n",
    "    out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "color_dirt = torch.tensor([0.3, 0.25, 0.2]).view(3,1,1)\n",
    "color_bruise = torch.tensor([0.25, 0.2, 0.15]).view(3,1,1)\n",
    "\n",
    "def noise_mild(x):\n",
    "    return clamp_01(x + torch.randn_like(x) * 0.025)\n",
    "\n",
    "def dark_mild(x):\n",
    "    return clamp_01(x * 0.65)\n",
    "\n",
    "def overexposed_mild(x):\n",
    "    return clamp_01(x * 1.35)\n",
    "\n",
    "def dirty_mild(x):\n",
    "    return add_color_patches(x, num_patches=2, color=color_dirt, alpha_range=(0.5, 0.8), size_range=(0.03, 0.08))\n",
    "\n",
    "def bruised_mild(x):\n",
    "    return add_color_patches(x, num_patches=1, color=color_bruise, alpha_range=(0.4, 0.7), size_range=(0.03, 0.08))\n",
    "\n",
    "def occlusion_small(x):\n",
    "    return add_occlusion_patch(x, area_ratio=0.10, alpha=0.5)\n",
    "\n",
    "blur_medium = T.GaussianBlur(kernel_size=5, sigma=1.0)\n",
    "\n",
    "def scenario_A(x):\n",
    "    x = blur_medium(x)\n",
    "    x = noise_mild(x)\n",
    "    if np.random.rand() < 0.7:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_B(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = dark_mild(x)\n",
    "    else:\n",
    "        x = overexposed_mild(x)\n",
    "    x = noise_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_C(x):\n",
    "    x = occlusion_small(x)\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = bruised_mild(x)\n",
    "    else:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "scenario_fns = {\n",
    "    \"scenario_A\": scenario_A,\n",
    "    \"scenario_B\": scenario_B,\n",
    "    \"scenario_C\": scenario_C,\n",
    "}\n",
    "\n",
    "scenarios_mixed = {\n",
    "    \"clean\": lambda x: x,\n",
    "    \"scenario_A\": scenario_A,\n",
    "    \"scenario_B\": scenario_B,\n",
    "    \"scenario_C\": scenario_C,\n",
    "}\n",
    "\n",
    "print(list(scenarios_mixed.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa0dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color histogram feature function ready\n"
     ]
    }
   ],
   "source": [
    "def color_hist_features(X_np, bins=COLOR_BINS, img_shape=(3, 32, 32)):\n",
    "    n_samples = X_np.shape[0]\n",
    "    feats = np.zeros((n_samples, 3 * bins), dtype=np.float32)\n",
    "    bin_edges = np.linspace(0.0, 1.0, bins + 1)\n",
    "    for i in range(n_samples):\n",
    "        img = X_np[i].reshape(img_shape)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "        img_hsv = (img * 255.0).astype(np.uint8)\n",
    "        img_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(img_hsv)\n",
    "        hists = []\n",
    "        for channel in (h, s, v):\n",
    "            ch_norm = channel.astype(np.float32) / 255.0\n",
    "            hist, _ = np.histogram(ch_norm.ravel(), bins=bin_edges, density=True)\n",
    "            hists.append(hist)\n",
    "        feats[i] = np.concatenate(hists)\n",
    "    return feats\n",
    "\n",
    "print(\"Color histogram feature function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6848d969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7 KNN checkpoint(s)\n",
      "Loaded 7 SVM checkpoint(s)\n"
     ]
    }
   ],
   "source": [
    "# Load KNN/SVM checkpoints\n",
    "ckpt_model_paths = sorted(CKPT_ROOT.rglob(\"model.joblib\"))\n",
    "if not ckpt_model_paths:\n",
    "    raise FileNotFoundError(f\"No checkpoints found under {CKPT_ROOT}\")\n",
    "\n",
    "def infer_model_noise_feature(run_dir: Path):\n",
    "    try:\n",
    "        rel_parts = run_dir.relative_to(CKPT_ROOT).parts\n",
    "    except ValueError:\n",
    "        rel_parts = run_dir.parts\n",
    "\n",
    "    model_idx = None\n",
    "    for i in range(len(rel_parts) - 1, -1, -1):\n",
    "        if rel_parts[i] in (\"knn\", \"svm\"):\n",
    "            model_idx = i\n",
    "            break\n",
    "\n",
    "    if model_idx is None:\n",
    "        return \"unknown\", \"unknown\", \"unknown\"\n",
    "\n",
    "    model_name = rel_parts[model_idx]\n",
    "    noise_name = rel_parts[model_idx - 1] if model_idx - 1 >= 0 else \"unknown\"\n",
    "    feature_name = rel_parts[model_idx - 2] if model_idx - 2 >= 0 else \"unknown\"\n",
    "    return model_name, noise_name, feature_name\n",
    "\n",
    "models = {}\n",
    "for model_path in ckpt_model_paths:\n",
    "    run_dir = model_path.parent\n",
    "    scaler_path = run_dir / \"scaler.joblib\"\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = joblib.load(scaler_path) if scaler_path.exists() else None\n",
    "    model_name, noise_name, feature_name = infer_model_noise_feature(run_dir)\n",
    "    \n",
    "    if model_name not in models:\n",
    "        models[model_name] = []\n",
    "    \n",
    "    models[model_name].append({\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"model\": model,\n",
    "        \"scaler\": scaler,\n",
    "        \"noise\": noise_name,\n",
    "        \"feature\": feature_name,\n",
    "    })\n",
    "\n",
    "knn_count = len(models.get(\"knn\", []))\n",
    "svm_count = len(models.get(\"svm\", []))\n",
    "unknown_count = len(models.get(\"unknown\", []))\n",
    "if knn_count == 0 and svm_count == 0:\n",
    "    raise FileNotFoundError(\"No KNN/SVM checkpoints found under artifacts/checkpoints/fruit360\")\n",
    "\n",
    "print(f\"Loaded {knn_count} KNN checkpoint(s)\")\n",
    "print(f\"Loaded {svm_count} SVM checkpoint(s)\")\n",
    "if unknown_count:\n",
    "    print(f\"Warning: {unknown_count} checkpoint(s) with unknown path layout\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd90df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing KNN checkpoint [1/7] - unknown/01_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9353\n",
      "Time: 33.02s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/01_noise/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9130    0.8182    0.8630        77\n",
      "           1     0.9024    0.9553    0.9281      5506\n",
      "           2     0.9873    0.9451    0.9657       164\n",
      "           3     0.9696    0.9735    0.9715      1017\n",
      "           4     0.8973    0.9209    0.9090       645\n",
      "           5     0.8462    1.0000    0.9167        77\n",
      "           6     0.8571    0.6400    0.7328       150\n",
      "           7     0.9724    0.9724    0.9724       145\n",
      "           8     0.9112    0.9233    0.9172       600\n",
      "           9     0.9808    0.9935    0.9871       154\n",
      "          10     1.0000    0.9062    0.9508        96\n",
      "          11     0.8306    0.9432    0.8833       634\n",
      "          12     0.9394    0.8267    0.8794        75\n",
      "          13     0.9736    0.9897    0.9816       484\n",
      "          14     0.9252    0.8193    0.8690       166\n",
      "          15     0.9592    0.9400    0.9495        50\n",
      "          16     0.8417    1.0000    0.9141       234\n",
      "          17     0.7436    0.9969    0.8518       320\n",
      "          18     0.9757    0.9872    0.9814      3048\n",
      "          19     0.8839    0.8954    0.8896       153\n",
      "          20     1.0000    0.9940    0.9970       166\n",
      "          21     0.9869    0.9096    0.9467       166\n",
      "          22     0.8722    0.5164    0.6488       304\n",
      "          23     0.9727    0.9651    0.9689      2065\n",
      "          24     0.9880    0.9880    0.9880       166\n",
      "          25     0.9613    0.7373    0.8345       236\n",
      "          26     0.9914    0.9872    0.9893       234\n",
      "          27     0.9005    0.7819    0.8370       243\n",
      "          28     0.9932    0.9545    0.9735       154\n",
      "          29     0.9419    0.9759    0.9586       166\n",
      "          30     0.9799    0.9762    0.9781      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     0.9939    0.9880    0.9909       166\n",
      "          33     0.9404    0.9045    0.9221       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     0.9595    1.0000    0.9794       166\n",
      "          36     0.7371    0.9167    0.8171       156\n",
      "          37     0.9861    0.9045    0.9435       157\n",
      "          38     0.9636    0.9578    0.9607       166\n",
      "          39     0.9640    0.9727    0.9683       330\n",
      "          40     0.9881    1.0000    0.9940       166\n",
      "          41     0.9873    0.9337    0.9598       166\n",
      "          42     0.9765    1.0000    0.9881       166\n",
      "          43     0.9216    0.9156    0.9186       308\n",
      "          44     0.8972    0.9412    0.9187       102\n",
      "          45     0.9294    0.9518    0.9405       166\n",
      "          46     0.9038    0.8780    0.8907       246\n",
      "          47     0.9939    1.0000    0.9970       164\n",
      "          48     0.8764    0.6688    0.7586       477\n",
      "          49     0.9049    0.9004    0.9027       793\n",
      "          50     0.9101    0.9301    0.9200      1045\n",
      "          51     0.9186    0.9875    0.9518       160\n",
      "          52     0.9642    0.8663    0.9126       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9128    0.9111    0.9119      1597\n",
      "          55     1.0000    0.7885    0.8817        52\n",
      "          56     0.8960    0.8938    0.8949      4087\n",
      "          57     0.9162    0.9217    0.9189       166\n",
      "          58     0.9626    0.9751    0.9688      1846\n",
      "          59     0.8982    0.9146    0.9063       328\n",
      "          60     0.8758    0.8571    0.8664       329\n",
      "          61     0.8819    0.9048    0.8932       231\n",
      "          62     0.9877    0.9639    0.9756       166\n",
      "          63     0.9665    0.9582    0.9623      1053\n",
      "          64     0.7212    0.4573    0.5597       164\n",
      "          65     0.9931    0.9346    0.9630       153\n",
      "          66     0.8906    0.7854    0.8347       601\n",
      "          67     0.9491    0.8930    0.9202       897\n",
      "          68     0.9937    0.9634    0.9783       164\n",
      "          69     0.9436    0.9844    0.9636       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     0.9873    0.9630    0.9750       162\n",
      "          72     0.9745    0.8966    0.9339       725\n",
      "          73     0.9755    0.9578    0.9666       166\n",
      "          74     0.9765    1.0000    0.9881       166\n",
      "          75     0.9777    0.9903    0.9840      3413\n",
      "          76     0.9920    1.0000    0.9960       249\n",
      "          77     0.9252    0.8662    0.8947       157\n",
      "          78     1.0000    0.9094    0.9526       254\n",
      "\n",
      "    accuracy                         0.9353     43442\n",
      "   macro avg     0.9394    0.9182    0.9261     43442\n",
      "weighted avg     0.9360    0.9353    0.9343     43442\n",
      "\n",
      "Testing KNN checkpoint [2/7] - unknown/02_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9497\n",
      "Time: 24.53s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/02_noise/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9861    0.9221    0.9530        77\n",
      "           1     0.9135    0.9704    0.9411      5506\n",
      "           2     1.0000    1.0000    1.0000       164\n",
      "           3     0.9775    0.9823    0.9799      1017\n",
      "           4     0.9434    0.9302    0.9368       645\n",
      "           5     0.9103    0.9221    0.9161        77\n",
      "           6     0.8468    0.6267    0.7203       150\n",
      "           7     0.9732    1.0000    0.9864       145\n",
      "           8     0.9728    0.9533    0.9630       600\n",
      "           9     0.9747    1.0000    0.9872       154\n",
      "          10     1.0000    0.9896    0.9948        96\n",
      "          11     0.8664    0.9511    0.9068       634\n",
      "          12     0.9571    0.8933    0.9241        75\n",
      "          13     0.9635    0.9814    0.9724       484\n",
      "          14     1.0000    0.8133    0.8970       166\n",
      "          15     0.9804    1.0000    0.9901        50\n",
      "          16     0.9070    1.0000    0.9512       234\n",
      "          17     0.7980    1.0000    0.8877       320\n",
      "          18     0.9840    0.9872    0.9856      3048\n",
      "          19     0.8931    0.9281    0.9103       153\n",
      "          20     0.9940    1.0000    0.9970       166\n",
      "          21     0.9874    0.9458    0.9662       166\n",
      "          22     0.9101    0.5329    0.6722       304\n",
      "          23     0.9770    0.9685    0.9728      2065\n",
      "          24     1.0000    0.9819    0.9909       166\n",
      "          25     0.9403    0.8008    0.8650       236\n",
      "          26     0.9873    1.0000    0.9936       234\n",
      "          27     0.9163    0.7654    0.8341       243\n",
      "          28     0.9935    0.9870    0.9902       154\n",
      "          29     0.9540    1.0000    0.9765       166\n",
      "          30     0.9900    0.9846    0.9873      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     0.9939    0.9880    0.9909       166\n",
      "          33     0.9259    0.9554    0.9404       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8639    0.9359    0.8985       156\n",
      "          37     0.9932    0.9236    0.9571       157\n",
      "          38     1.0000    0.9578    0.9785       166\n",
      "          39     0.9879    0.9909    0.9894       330\n",
      "          40     1.0000    0.9940    0.9970       166\n",
      "          41     0.9881    1.0000    0.9940       166\n",
      "          42     0.9477    0.9819    0.9645       166\n",
      "          43     0.9693    0.9221    0.9451       308\n",
      "          44     0.8571    1.0000    0.9231       102\n",
      "          45     0.9595    1.0000    0.9794       166\n",
      "          46     0.9065    0.9065    0.9065       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9282    0.6771    0.7830       477\n",
      "          49     0.9463    0.9117    0.9287       793\n",
      "          50     0.9112    0.9818    0.9452      1045\n",
      "          51     0.9639    1.0000    0.9816       160\n",
      "          52     0.9678    0.8936    0.9292       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9355    0.9267    0.9311      1597\n",
      "          55     1.0000    0.9231    0.9600        52\n",
      "          56     0.9178    0.9092    0.9135      4087\n",
      "          57     0.9096    0.9699    0.9388       166\n",
      "          58     0.9805    0.9805    0.9805      1846\n",
      "          59     0.9050    0.9299    0.9173       328\n",
      "          60     0.8779    0.9179    0.8975       329\n",
      "          61     0.8996    0.9697    0.9333       231\n",
      "          62     1.0000    0.9759    0.9878       166\n",
      "          63     0.9800    0.9763    0.9781      1053\n",
      "          64     0.6724    0.4756    0.5571       164\n",
      "          65     0.9934    0.9869    0.9902       153\n",
      "          66     0.9272    0.7837    0.8494       601\n",
      "          67     0.9561    0.9465    0.9513       897\n",
      "          68     1.0000    0.9939    0.9969       164\n",
      "          69     0.9380    0.9988    0.9674       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9568    0.9779       162\n",
      "          72     0.9751    0.9172    0.9453       725\n",
      "          73     0.9760    0.9819    0.9790       166\n",
      "          74     0.9432    1.0000    0.9708       166\n",
      "          75     0.9863    0.9912    0.9887      3413\n",
      "          76     0.9920    1.0000    0.9960       249\n",
      "          77     0.9222    0.9809    0.9506       157\n",
      "          78     0.9916    0.9252    0.9572       254\n",
      "\n",
      "    accuracy                         0.9497     43442\n",
      "   macro avg     0.9530    0.9400    0.9443     43442\n",
      "weighted avg     0.9501    0.9497    0.9486     43442\n",
      "\n",
      "Testing KNN checkpoint [3/7] - unknown/03_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9571\n",
      "Time: 25.26s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/03_noise/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9737    0.9610    0.9673        77\n",
      "           1     0.9280    0.9731    0.9500      5506\n",
      "           2     0.9879    0.9939    0.9909       164\n",
      "           3     0.9739    0.9902    0.9820      1017\n",
      "           4     0.9376    0.9318    0.9347       645\n",
      "           5     0.9625    1.0000    0.9809        77\n",
      "           6     0.8713    0.5867    0.7012       150\n",
      "           7     0.9864    1.0000    0.9932       145\n",
      "           8     0.9534    0.9550    0.9542       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.8743    0.9763    0.9225       634\n",
      "          12     0.9722    0.9333    0.9524        75\n",
      "          13     0.9527    0.9979    0.9748       484\n",
      "          14     1.0000    0.8434    0.9150       166\n",
      "          15     0.9615    1.0000    0.9804        50\n",
      "          16     0.9035    1.0000    0.9493       234\n",
      "          17     0.8247    1.0000    0.9040       320\n",
      "          18     0.9954    0.9843    0.9898      3048\n",
      "          19     0.9030    0.9739    0.9371       153\n",
      "          20     1.0000    0.9940    0.9970       166\n",
      "          21     0.9936    0.9337    0.9627       166\n",
      "          22     0.9357    0.5263    0.6737       304\n",
      "          23     0.9780    0.9705    0.9742      2065\n",
      "          24     0.9759    0.9759    0.9759       166\n",
      "          25     0.9795    0.8093    0.8863       236\n",
      "          26     0.9829    0.9829    0.9829       234\n",
      "          27     0.9663    0.8272    0.8914       243\n",
      "          28     0.9935    1.0000    0.9968       154\n",
      "          29     0.9651    1.0000    0.9822       166\n",
      "          30     0.9946    0.9954    0.9950      1301\n",
      "          31     0.9970    1.0000    0.9985       330\n",
      "          32     0.9940    1.0000    0.9970       166\n",
      "          33     0.9096    0.9618    0.9350       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    0.9940    0.9970       166\n",
      "          36     0.8571    0.9231    0.8889       156\n",
      "          37     0.9930    0.8981    0.9431       157\n",
      "          38     1.0000    0.9639    0.9816       166\n",
      "          39     0.9939    0.9879    0.9909       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     0.9881    1.0000    0.9940       166\n",
      "          42     0.9379    1.0000    0.9679       166\n",
      "          43     0.9864    0.9416    0.9635       308\n",
      "          44     0.9027    1.0000    0.9488       102\n",
      "          45     0.9704    0.9880    0.9791       166\n",
      "          46     0.8959    0.9797    0.9359       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.8946    0.6939    0.7816       477\n",
      "          49     0.9505    0.9206    0.9353       793\n",
      "          50     0.9234    0.9809    0.9513      1045\n",
      "          51     0.9815    0.9938    0.9876       160\n",
      "          52     0.9768    0.9381    0.9571       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9437    0.9543    0.9489      1597\n",
      "          55     1.0000    0.9423    0.9703        52\n",
      "          56     0.9365    0.9232    0.9298      4087\n",
      "          57     0.9357    0.9639    0.9496       166\n",
      "          58     0.9838    0.9854    0.9846      1846\n",
      "          59     0.9116    0.9116    0.9116       328\n",
      "          60     0.8768    0.9514    0.9125       329\n",
      "          61     0.9157    0.9870    0.9500       231\n",
      "          62     1.0000    0.9819    0.9909       166\n",
      "          63     0.9847    0.9810    0.9829      1053\n",
      "          64     0.7656    0.5976    0.6712       164\n",
      "          65     1.0000    0.9869    0.9934       153\n",
      "          66     0.9545    0.8020    0.8716       601\n",
      "          67     0.9676    0.9666    0.9671       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9432    0.9976    0.9697       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9444    0.9714       162\n",
      "          72     0.9764    0.9117    0.9429       725\n",
      "          73     0.9758    0.9699    0.9728       166\n",
      "          74     0.9540    1.0000    0.9765       166\n",
      "          75     0.9858    0.9956    0.9907      3413\n",
      "          76     0.9842    1.0000    0.9920       249\n",
      "          77     0.9394    0.9873    0.9627       157\n",
      "          78     1.0000    0.9291    0.9633       254\n",
      "\n",
      "    accuracy                         0.9571     43442\n",
      "   macro avg     0.9597    0.9488    0.9520     43442\n",
      "weighted avg     0.9576    0.9571    0.9561     43442\n",
      "\n",
      "Testing KNN checkpoint [4/7] - unknown/color_hist...\n",
      "\n",
      "Accuracy on mixed test: 0.8183\n",
      "Time: 24.54s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/color_hist/knn/20260211-114401_efd2c1b4\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6883    0.8154        77\n",
      "           1     0.8387    0.8592    0.8488      5506\n",
      "           2     0.8383    0.8537    0.8459       164\n",
      "           3     0.9385    0.6903    0.7955      1017\n",
      "           4     0.6538    0.8434    0.7366       645\n",
      "           5     0.9123    0.6753    0.7761        77\n",
      "           6     0.9250    0.4933    0.6435       150\n",
      "           7     1.0000    0.9172    0.9568       145\n",
      "           8     0.9573    0.7100    0.8153       600\n",
      "           9     1.0000    0.8182    0.9000       154\n",
      "          10     0.4500    0.9375    0.6081        96\n",
      "          11     0.7541    0.7981    0.7755       634\n",
      "          12     0.8493    0.8267    0.8378        75\n",
      "          13     0.9621    0.8905    0.9249       484\n",
      "          14     0.8803    0.7530    0.8117       166\n",
      "          15     0.9394    0.6200    0.7470        50\n",
      "          16     0.8987    0.9103    0.9045       234\n",
      "          17     0.8732    0.7750    0.8212       320\n",
      "          18     0.8885    0.8501    0.8689      3048\n",
      "          19     0.6029    0.8039    0.6891       153\n",
      "          20     0.6895    0.9096    0.7844       166\n",
      "          21     1.0000    0.6687    0.8014       166\n",
      "          22     0.7337    0.4441    0.5533       304\n",
      "          23     0.9386    0.7995    0.8635      2065\n",
      "          24     0.8551    0.7108    0.7763       166\n",
      "          25     0.7923    0.6144    0.6921       236\n",
      "          26     0.7804    0.9872    0.8717       234\n",
      "          27     0.9610    0.6091    0.7456       243\n",
      "          28     1.0000    0.7013    0.8244       154\n",
      "          29     0.9660    0.8554    0.9073       166\n",
      "          30     0.8567    0.8132    0.8344      1301\n",
      "          31     0.9685    0.8394    0.8994       330\n",
      "          32     0.7637    0.8373    0.7989       166\n",
      "          33     0.8636    0.7261    0.7889       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     0.7812    0.9036    0.8380       166\n",
      "          36     0.9057    0.6154    0.7328       156\n",
      "          37     0.9815    0.6752    0.8000       157\n",
      "          38     0.6712    0.8976    0.7680       166\n",
      "          39     0.9244    0.8152    0.8663       330\n",
      "          40     0.9868    0.9036    0.9434       166\n",
      "          41     1.0000    0.7892    0.8822       166\n",
      "          42     0.9932    0.8795    0.9329       166\n",
      "          43     0.8317    0.8182    0.8249       308\n",
      "          44     0.8519    0.9020    0.8762       102\n",
      "          45     0.9172    0.8675    0.8916       166\n",
      "          46     0.6573    0.7642    0.7068       246\n",
      "          47     1.0000    0.7378    0.8491       164\n",
      "          48     0.7983    0.5891    0.6779       477\n",
      "          49     0.8694    0.7301    0.7937       793\n",
      "          50     0.7458    0.8593    0.7986      1045\n",
      "          51     1.0000    0.8438    0.9153       160\n",
      "          52     0.8753    0.8168    0.8451       404\n",
      "          53     0.6407    0.8916    0.7456       166\n",
      "          54     0.8330    0.7965    0.8143      1597\n",
      "          55     1.0000    0.7500    0.8571        52\n",
      "          56     0.7388    0.8566    0.7933      4087\n",
      "          57     0.7105    0.8133    0.7584       166\n",
      "          58     0.5704    0.9090    0.7009      1846\n",
      "          59     0.7814    0.7409    0.7606       328\n",
      "          60     0.7611    0.7842    0.7725       329\n",
      "          61     0.9432    0.7186    0.8157       231\n",
      "          62     0.4602    0.9398    0.6178       166\n",
      "          63     0.8526    0.7854    0.8176      1053\n",
      "          64     0.5000    0.5000    0.5000       164\n",
      "          65     0.8790    0.9020    0.8903       153\n",
      "          66     0.9501    0.6656    0.7828       601\n",
      "          67     0.8696    0.7581    0.8100       897\n",
      "          68     0.8808    0.8110    0.8444       164\n",
      "          69     0.9604    0.8739    0.9151       833\n",
      "          70     1.0000    0.8598    0.9246       164\n",
      "          71     0.7899    0.6728    0.7267       162\n",
      "          72     0.8876    0.8386    0.8624       725\n",
      "          73     0.7407    0.8434    0.7887       166\n",
      "          74     0.9928    0.8313    0.9049       166\n",
      "          75     0.8897    0.8579    0.8735      3413\n",
      "          76     1.0000    0.8795    0.9359       249\n",
      "          77     0.7644    0.9299    0.8391       157\n",
      "          78     0.6110    0.8780    0.7205       254\n",
      "\n",
      "    accuracy                         0.8183     43442\n",
      "   macro avg     0.8498    0.7940    0.8099     43442\n",
      "weighted avg     0.8375    0.8183    0.8207     43442\n",
      "\n",
      "Testing KNN checkpoint [5/7] - unknown/heavy_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9602\n",
      "Time: 25.67s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/heavy_augmentation/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9610    0.9801        77\n",
      "           1     0.9356    0.9738    0.9543      5506\n",
      "           2     0.9818    0.9878    0.9848       164\n",
      "           3     0.9805    0.9912    0.9858      1017\n",
      "           4     0.9447    0.9535    0.9491       645\n",
      "           5     0.9506    1.0000    0.9747        77\n",
      "           6     0.9208    0.6200    0.7410       150\n",
      "           7     0.9864    1.0000    0.9932       145\n",
      "           8     0.9668    0.9700    0.9684       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9136    0.9842    0.9476       634\n",
      "          12     0.9730    0.9600    0.9664        75\n",
      "          13     0.9470    0.9959    0.9708       484\n",
      "          14     0.9928    0.8313    0.9049       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9176    1.0000    0.9571       234\n",
      "          17     0.8020    1.0000    0.8901       320\n",
      "          18     0.9951    0.9898    0.9924      3048\n",
      "          19     0.8868    0.9216    0.9038       153\n",
      "          20     1.0000    0.9940    0.9970       166\n",
      "          21     0.9691    0.9458    0.9573       166\n",
      "          22     0.9412    0.5263    0.6751       304\n",
      "          23     0.9816    0.9801    0.9809      2065\n",
      "          24     1.0000    0.9940    0.9970       166\n",
      "          25     0.9895    0.8008    0.8852       236\n",
      "          26     0.9957    1.0000    0.9979       234\n",
      "          27     0.9505    0.8683    0.9075       243\n",
      "          28     0.9935    1.0000    0.9968       154\n",
      "          29     0.9540    1.0000    0.9765       166\n",
      "          30     0.9931    0.9946    0.9939      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     1.0000    1.0000    1.0000       166\n",
      "          33     0.9250    0.9427    0.9338       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8970    0.9487    0.9221       156\n",
      "          37     0.9934    0.9554    0.9740       157\n",
      "          38     1.0000    0.9578    0.9785       166\n",
      "          39     0.9970    0.9909    0.9939       330\n",
      "          40     1.0000    0.9880    0.9939       166\n",
      "          41     0.9881    1.0000    0.9940       166\n",
      "          42     0.9375    0.9940    0.9649       166\n",
      "          43     0.9508    0.9416    0.9462       308\n",
      "          44     0.8571    1.0000    0.9231       102\n",
      "          45     0.9591    0.9880    0.9733       166\n",
      "          46     0.9264    0.9715    0.9484       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9073    0.6771    0.7755       477\n",
      "          49     0.9487    0.9319    0.9402       793\n",
      "          50     0.9300    0.9789    0.9538      1045\n",
      "          51     0.9816    1.0000    0.9907       160\n",
      "          52     0.9847    0.9579    0.9711       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9446    0.9499    0.9472      1597\n",
      "          55     0.9245    0.9423    0.9333        52\n",
      "          56     0.9406    0.9295    0.9350      4087\n",
      "          57     0.9360    0.9699    0.9527       166\n",
      "          58     0.9838    0.9881    0.9859      1846\n",
      "          59     0.8958    0.9177    0.9066       328\n",
      "          60     0.8835    0.9453    0.9134       329\n",
      "          61     0.9153    0.9827    0.9478       231\n",
      "          62     1.0000    0.9759    0.9878       166\n",
      "          63     0.9876    0.9839    0.9857      1053\n",
      "          64     0.7634    0.6098    0.6780       164\n",
      "          65     1.0000    0.9935    0.9967       153\n",
      "          66     0.9575    0.8253    0.8865       601\n",
      "          67     0.9674    0.9588    0.9630       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9432    0.9976    0.9697       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9444    0.9714       162\n",
      "          72     0.9824    0.9241    0.9524       725\n",
      "          73     0.9879    0.9819    0.9849       166\n",
      "          74     0.9379    1.0000    0.9679       166\n",
      "          75     0.9875    0.9953    0.9914      3413\n",
      "          76     0.9803    1.0000    0.9901       249\n",
      "          77     0.9231    0.9172    0.9201       157\n",
      "          78     1.0000    0.9370    0.9675       254\n",
      "\n",
      "    accuracy                         0.9602     43442\n",
      "   macro avg     0.9606    0.9511    0.9537     43442\n",
      "weighted avg     0.9605    0.9602    0.9592     43442\n",
      "\n",
      "Testing KNN checkpoint [6/7] - unknown/light_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9574\n",
      "Time: 25.48s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/light_augmentation/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9870    0.9870    0.9870        77\n",
      "           1     0.9257    0.9728    0.9486      5506\n",
      "           2     0.9938    0.9817    0.9877       164\n",
      "           3     0.9824    0.9862    0.9843      1017\n",
      "           4     0.9581    0.9581    0.9581       645\n",
      "           5     0.9747    1.0000    0.9872        77\n",
      "           6     0.8902    0.4867    0.6293       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9668    0.9717    0.9692       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.8741    0.9858    0.9266       634\n",
      "          12     0.9861    0.9467    0.9660        75\n",
      "          13     0.9583    0.9979    0.9777       484\n",
      "          14     0.9789    0.8373    0.9026       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9176    1.0000    0.9571       234\n",
      "          17     0.7838    0.9969    0.8776       320\n",
      "          18     0.9908    0.9879    0.9893      3048\n",
      "          19     0.9125    0.9542    0.9329       153\n",
      "          20     1.0000    0.9940    0.9970       166\n",
      "          21     0.9814    0.9518    0.9664       166\n",
      "          22     0.9512    0.5132    0.6667       304\n",
      "          23     0.9872    0.9724    0.9798      2065\n",
      "          24     1.0000    0.9518    0.9753       166\n",
      "          25     0.9845    0.8093    0.8884       236\n",
      "          26     0.9669    1.0000    0.9832       234\n",
      "          27     0.9352    0.8313    0.8802       243\n",
      "          28     0.9935    1.0000    0.9968       154\n",
      "          29     0.9651    1.0000    0.9822       166\n",
      "          30     0.9893    0.9939    0.9916      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     0.9940    0.9940    0.9940       166\n",
      "          33     0.9390    0.9809    0.9595       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8483    0.9679    0.9042       156\n",
      "          37     1.0000    0.9490    0.9739       157\n",
      "          38     1.0000    0.9639    0.9816       166\n",
      "          39     0.9909    0.9939    0.9924       330\n",
      "          40     1.0000    0.9819    0.9909       166\n",
      "          41     0.9940    1.0000    0.9970       166\n",
      "          42     0.9425    0.9880    0.9647       166\n",
      "          43     0.9564    0.9253    0.9406       308\n",
      "          44     0.9266    0.9902    0.9573       102\n",
      "          45     0.9634    0.9518    0.9576       166\n",
      "          46     0.9004    0.9187    0.9095       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.8930    0.7002    0.7850       477\n",
      "          49     0.9445    0.9231    0.9337       793\n",
      "          50     0.9380    0.9837    0.9603      1045\n",
      "          51     0.9938    1.0000    0.9969       160\n",
      "          52     0.9795    0.9480    0.9635       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9442    0.9424    0.9433      1597\n",
      "          55     0.8750    0.9423    0.9074        52\n",
      "          56     0.9406    0.9264    0.9334      4087\n",
      "          57     0.8889    0.9639    0.9249       166\n",
      "          58     0.9822    0.9854    0.9838      1846\n",
      "          59     0.9009    0.8872    0.8940       328\n",
      "          60     0.8908    0.9422    0.9158       329\n",
      "          61     0.9036    0.9740    0.9375       231\n",
      "          62     0.9939    0.9759    0.9848       166\n",
      "          63     0.9839    0.9839    0.9839      1053\n",
      "          64     0.7200    0.5488    0.6228       164\n",
      "          65     1.0000    0.9869    0.9934       153\n",
      "          66     0.9400    0.8087    0.8694       601\n",
      "          67     0.9773    0.9588    0.9679       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9369    0.9976    0.9663       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    1.0000    1.0000       162\n",
      "          72     0.9853    0.9214    0.9522       725\n",
      "          73     0.9817    0.9699    0.9758       166\n",
      "          74     0.9486    1.0000    0.9736       166\n",
      "          75     0.9855    0.9938    0.9896      3413\n",
      "          76     0.9920    1.0000    0.9960       249\n",
      "          77     0.9235    1.0000    0.9602       157\n",
      "          78     0.9959    0.9567    0.9759       254\n",
      "\n",
      "    accuracy                         0.9574     43442\n",
      "   macro avg     0.9586    0.9480    0.9507     43442\n",
      "weighted avg     0.9579    0.9574    0.9563     43442\n",
      "\n",
      "Testing KNN checkpoint [7/7] - unknown/medium_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9590\n",
      "Time: 24.70s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/medium_augmentation/knn\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9870    0.9935        77\n",
      "           1     0.9278    0.9735    0.9501      5506\n",
      "           2     0.9939    0.9878    0.9908       164\n",
      "           3     0.9804    0.9862    0.9833      1017\n",
      "           4     0.9563    0.9504    0.9533       645\n",
      "           5     0.9747    1.0000    0.9872        77\n",
      "           6     0.8515    0.5733    0.6853       150\n",
      "           7     0.9864    1.0000    0.9932       145\n",
      "           8     0.9731    0.9650    0.9690       600\n",
      "           9     0.9935    1.0000    0.9968       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.8722    0.9685    0.9178       634\n",
      "          12     0.9861    0.9467    0.9660        75\n",
      "          13     0.9489    0.9979    0.9728       484\n",
      "          14     0.9856    0.8253    0.8984       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9070    1.0000    0.9512       234\n",
      "          17     0.8142    1.0000    0.8976       320\n",
      "          18     0.9934    0.9892    0.9913      3048\n",
      "          19     0.8659    0.9281    0.8959       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     0.9873    0.9398    0.9630       166\n",
      "          22     0.9591    0.5395    0.6905       304\n",
      "          23     0.9821    0.9811    0.9816      2065\n",
      "          24     1.0000    1.0000    1.0000       166\n",
      "          25     0.9746    0.8136    0.8868       236\n",
      "          26     1.0000    1.0000    1.0000       234\n",
      "          27     0.9484    0.8313    0.8860       243\n",
      "          28     0.9935    1.0000    0.9968       154\n",
      "          29     0.9379    1.0000    0.9679       166\n",
      "          30     0.9923    0.9939    0.9931      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     1.0000    1.0000    1.0000       166\n",
      "          33     0.9264    0.9618    0.9437       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8869    0.9551    0.9198       156\n",
      "          37     1.0000    0.9682    0.9838       157\n",
      "          38     1.0000    0.9398    0.9689       166\n",
      "          39     0.9970    0.9939    0.9954       330\n",
      "          40     1.0000    0.9880    0.9939       166\n",
      "          41     1.0000    1.0000    1.0000       166\n",
      "          42     0.9486    1.0000    0.9736       166\n",
      "          43     0.9664    0.9351    0.9505       308\n",
      "          44     0.8938    0.9902    0.9395       102\n",
      "          45     0.9704    0.9880    0.9791       166\n",
      "          46     0.9234    0.9797    0.9507       246\n",
      "          47     0.9939    1.0000    0.9970       164\n",
      "          48     0.9112    0.6667    0.7700       477\n",
      "          49     0.9514    0.9142    0.9325       793\n",
      "          50     0.9256    0.9885    0.9560      1045\n",
      "          51     0.9816    1.0000    0.9907       160\n",
      "          52     0.9772    0.9530    0.9649       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9430    0.9537    0.9483      1597\n",
      "          55     0.9231    0.9231    0.9231        52\n",
      "          56     0.9466    0.9273    0.9368      4087\n",
      "          57     0.9415    0.9699    0.9555       166\n",
      "          58     0.9864    0.9805    0.9834      1846\n",
      "          59     0.9152    0.9207    0.9179       328\n",
      "          60     0.8860    0.9453    0.9147       329\n",
      "          61     0.9113    0.9784    0.9436       231\n",
      "          62     1.0000    0.9819    0.9909       166\n",
      "          63     0.9885    0.9791    0.9838      1053\n",
      "          64     0.7080    0.5915    0.6445       164\n",
      "          65     0.9934    0.9804    0.9868       153\n",
      "          66     0.9488    0.8020    0.8693       601\n",
      "          67     0.9634    0.9688    0.9661       897\n",
      "          68     1.0000    0.9939    0.9969       164\n",
      "          69     0.9486    0.9976    0.9725       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9691    0.9843       162\n",
      "          72     0.9842    0.9434    0.9634       725\n",
      "          73     0.9527    0.9699    0.9612       166\n",
      "          74     0.9708    1.0000    0.9852       166\n",
      "          75     0.9844    0.9953    0.9898      3413\n",
      "          76     0.9920    1.0000    0.9960       249\n",
      "          77     0.9259    0.9554    0.9404       157\n",
      "          78     1.0000    0.9291    0.9633       254\n",
      "\n",
      "    accuracy                         0.9590     43442\n",
      "   macro avg     0.9602    0.9501    0.9530     43442\n",
      "weighted avg     0.9595    0.9590    0.9580     43442\n",
      "\n",
      "Testing SVM checkpoint [1/7] - unknown/01_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9583\n",
      "Time: 97.48s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/01_noise/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9865    0.9481    0.9669        77\n",
      "           1     0.9214    0.9671    0.9437      5506\n",
      "           2     0.9939    0.9939    0.9939       164\n",
      "           3     0.9754    0.9744    0.9749      1017\n",
      "           4     0.9335    0.9581    0.9457       645\n",
      "           5     0.7778    1.0000    0.8750        77\n",
      "           6     0.9245    0.6533    0.7656       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9572    0.9683    0.9627       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     0.9796    1.0000    0.9897        96\n",
      "          11     0.8810    0.9811    0.9284       634\n",
      "          12     1.0000    0.9333    0.9655        75\n",
      "          13     0.9959    0.9959    0.9959       484\n",
      "          14     0.9551    0.8976    0.9255       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9551    1.0000    0.9770       234\n",
      "          17     0.8092    0.9938    0.8920       320\n",
      "          18     0.9896    0.9984    0.9940      3048\n",
      "          19     0.8935    0.9869    0.9379       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     1.0000    0.9337    0.9657       166\n",
      "          22     0.9611    0.5691    0.7149       304\n",
      "          23     0.9835    0.9801    0.9818      2065\n",
      "          24     0.9765    1.0000    0.9881       166\n",
      "          25     0.9781    0.7585    0.8544       236\n",
      "          26     1.0000    0.9957    0.9979       234\n",
      "          27     0.9545    0.8642    0.9071       243\n",
      "          28     0.9679    0.9805    0.9742       154\n",
      "          29     0.9762    0.9880    0.9820       166\n",
      "          30     0.9862    0.9892    0.9877      1301\n",
      "          31     0.9910    1.0000    0.9955       330\n",
      "          32     0.9939    0.9819    0.9879       166\n",
      "          33     0.9321    0.9618    0.9467       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.7876    0.9744    0.8711       156\n",
      "          37     1.0000    0.9554    0.9772       157\n",
      "          38     0.9878    0.9759    0.9818       166\n",
      "          39     0.9875    0.9606    0.9739       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     0.9873    0.9337    0.9598       166\n",
      "          42     0.9765    1.0000    0.9881       166\n",
      "          43     0.9928    0.8961    0.9420       308\n",
      "          44     0.8889    0.9412    0.9143       102\n",
      "          45     0.9939    0.9880    0.9909       166\n",
      "          46     0.9792    0.9553    0.9671       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.8681    0.7589    0.8098       477\n",
      "          49     0.9383    0.9016    0.9196       793\n",
      "          50     0.9413    0.9818    0.9611      1045\n",
      "          51     0.9938    1.0000    0.9969       160\n",
      "          52     0.9753    0.9777    0.9765       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9381    0.9393    0.9387      1597\n",
      "          55     0.9778    0.8462    0.9072        52\n",
      "          56     0.9315    0.9178    0.9246      4087\n",
      "          57     0.9704    0.9880    0.9791       166\n",
      "          58     0.9902    0.9870    0.9886      1846\n",
      "          59     0.9789    0.9878    0.9833       328\n",
      "          60     0.9167    0.9696    0.9424       329\n",
      "          61     0.9036    0.9740    0.9375       231\n",
      "          62     0.9939    0.9819    0.9879       166\n",
      "          63     0.9816    0.9649    0.9732      1053\n",
      "          64     0.8762    0.5610    0.6840       164\n",
      "          65     0.9933    0.9673    0.9801       153\n",
      "          66     0.9802    0.8253    0.8961       601\n",
      "          67     0.9825    0.9365    0.9589       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9465    0.9988    0.9720       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9877    0.9938       162\n",
      "          72     0.9736    0.9655    0.9695       725\n",
      "          73     0.9758    0.9699    0.9728       166\n",
      "          74     1.0000    1.0000    1.0000       166\n",
      "          75     0.9863    0.9906    0.9885      3413\n",
      "          76     0.9881    1.0000    0.9940       249\n",
      "          77     0.9806    0.9682    0.9744       157\n",
      "          78     0.9764    0.9764    0.9764       254\n",
      "\n",
      "    accuracy                         0.9583     43442\n",
      "   macro avg     0.9642    0.9514    0.9553     43442\n",
      "weighted avg     0.9591    0.9583    0.9576     43442\n",
      "\n",
      "Testing SVM checkpoint [2/7] - unknown/02_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9668\n",
      "Time: 109.30s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/02_noise/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9740    0.9868        77\n",
      "           1     0.9321    0.9718    0.9515      5506\n",
      "           2     0.9820    1.0000    0.9909       164\n",
      "           3     0.9863    0.9882    0.9872      1017\n",
      "           4     0.9407    0.9829    0.9613       645\n",
      "           5     0.9277    1.0000    0.9625        77\n",
      "           6     0.8644    0.6800    0.7612       150\n",
      "           7     0.9864    1.0000    0.9932       145\n",
      "           8     0.9731    0.9650    0.9690       600\n",
      "           9     0.9935    1.0000    0.9968       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9116    0.9921    0.9502       634\n",
      "          12     1.0000    0.9467    0.9726        75\n",
      "          13     1.0000    0.9959    0.9979       484\n",
      "          14     0.9536    0.8675    0.9085       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9512    1.0000    0.9750       234\n",
      "          17     0.8373    0.9969    0.9101       320\n",
      "          18     0.9932    0.9997    0.9964      3048\n",
      "          19     0.9264    0.9869    0.9557       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     0.9937    0.9458    0.9691       166\n",
      "          22     0.8980    0.5789    0.7040       304\n",
      "          23     0.9912    0.9821    0.9866      2065\n",
      "          24     0.9708    1.0000    0.9852       166\n",
      "          25     0.9948    0.8136    0.8951       236\n",
      "          26     0.9957    1.0000    0.9979       234\n",
      "          27     0.9813    0.8642    0.9190       243\n",
      "          28     1.0000    1.0000    1.0000       154\n",
      "          29     0.9881    1.0000    0.9940       166\n",
      "          30     0.9893    0.9985    0.9939      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     0.9940    1.0000    0.9970       166\n",
      "          33     0.9337    0.9873    0.9598       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8492    0.9744    0.9075       156\n",
      "          37     1.0000    0.9745    0.9871       157\n",
      "          38     0.9940    0.9940    0.9940       166\n",
      "          39     0.9939    0.9879    0.9909       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     1.0000    1.0000    1.0000       166\n",
      "          42     0.9708    1.0000    0.9852       166\n",
      "          43     0.9930    0.9156    0.9527       308\n",
      "          44     0.9623    1.0000    0.9808       102\n",
      "          45     0.9881    1.0000    0.9940       166\n",
      "          46     0.9958    0.9675    0.9814       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.8921    0.8323    0.8612       477\n",
      "          49     0.9554    0.9445    0.9499       793\n",
      "          50     0.9510    0.9837    0.9671      1045\n",
      "          51     1.0000    1.0000    1.0000       160\n",
      "          52     0.9656    0.9728    0.9692       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9515    0.9455    0.9485      1597\n",
      "          55     0.9600    0.9231    0.9412        52\n",
      "          56     0.9410    0.9215    0.9311      4087\n",
      "          57     0.9820    0.9880    0.9850       166\n",
      "          58     0.9930    0.9930    0.9930      1846\n",
      "          59     0.9873    0.9482    0.9673       328\n",
      "          60     0.9353    0.9666    0.9507       329\n",
      "          61     0.9197    0.9913    0.9542       231\n",
      "          62     1.0000    0.9940    0.9970       166\n",
      "          63     0.9894    0.9763    0.9828      1053\n",
      "          64     0.8718    0.6220    0.7260       164\n",
      "          65     1.0000    1.0000    1.0000       153\n",
      "          66     0.9781    0.8902    0.9321       601\n",
      "          67     0.9754    0.9744    0.9749       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9742    0.9988    0.9864       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     0.9815    0.9815    0.9815       162\n",
      "          72     0.9859    0.9628    0.9742       725\n",
      "          73     0.9880    0.9880    0.9880       166\n",
      "          74     0.9940    1.0000    0.9970       166\n",
      "          75     0.9938    0.9915    0.9927      3413\n",
      "          76     0.9920    1.0000    0.9960       249\n",
      "          77     0.9573    1.0000    0.9782       157\n",
      "          78     0.9960    0.9803    0.9881       254\n",
      "\n",
      "    accuracy                         0.9668     43442\n",
      "   macro avg     0.9715    0.9633    0.9660     43442\n",
      "weighted avg     0.9670    0.9668    0.9662     43442\n",
      "\n",
      "Testing SVM checkpoint [3/7] - unknown/03_noise...\n",
      "\n",
      "Accuracy on mixed test: 0.9705\n",
      "Time: 119.08s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/03_noise/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9747    1.0000    0.9872        77\n",
      "           1     0.9317    0.9788    0.9547      5506\n",
      "           2     0.9820    1.0000    0.9909       164\n",
      "           3     0.9912    0.9931    0.9921      1017\n",
      "           4     0.9531    0.9767    0.9648       645\n",
      "           5     0.9167    1.0000    0.9565        77\n",
      "           6     0.9245    0.6533    0.7656       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9784    0.9817    0.9800       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9385    0.9874    0.9623       634\n",
      "          12     0.9867    0.9867    0.9867        75\n",
      "          13     1.0000    0.9979    0.9990       484\n",
      "          14     0.9799    0.8795    0.9270       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9435    1.0000    0.9710       234\n",
      "          17     0.8552    0.9969    0.9206       320\n",
      "          18     0.9951    0.9993    0.9972      3048\n",
      "          19     0.9107    1.0000    0.9533       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     1.0000    0.9518    0.9753       166\n",
      "          22     0.9615    0.5757    0.7202       304\n",
      "          23     0.9846    0.9889    0.9867      2065\n",
      "          24     1.0000    1.0000    1.0000       166\n",
      "          25     0.9895    0.7966    0.8826       236\n",
      "          26     1.0000    1.0000    1.0000       234\n",
      "          27     0.9814    0.8683    0.9214       243\n",
      "          28     0.9872    1.0000    0.9935       154\n",
      "          29     0.9940    1.0000    0.9970       166\n",
      "          30     0.9946    1.0000    0.9973      1301\n",
      "          31     0.9970    1.0000    0.9985       330\n",
      "          32     1.0000    0.9940    0.9970       166\n",
      "          33     0.9620    0.9682    0.9651       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.9226    0.9936    0.9568       156\n",
      "          37     1.0000    0.9936    0.9968       157\n",
      "          38     0.9880    0.9940    0.9910       166\n",
      "          39     0.9970    0.9909    0.9939       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     1.0000    0.9940    0.9970       166\n",
      "          42     0.9708    1.0000    0.9852       166\n",
      "          43     0.9966    0.9416    0.9683       308\n",
      "          44     0.9800    0.9608    0.9703       102\n",
      "          45     1.0000    1.0000    1.0000       166\n",
      "          46     1.0000    0.9797    0.9897       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9384    0.7987    0.8630       477\n",
      "          49     0.9816    0.9433    0.9621       793\n",
      "          50     0.9281    0.9885    0.9574      1045\n",
      "          51     1.0000    0.9938    0.9969       160\n",
      "          52     0.9900    0.9777    0.9838       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9457    0.9606    0.9531      1597\n",
      "          55     0.9796    0.9231    0.9505        52\n",
      "          56     0.9556    0.9317    0.9435      4087\n",
      "          57     0.9938    0.9639    0.9786       166\n",
      "          58     0.9930    0.9935    0.9932      1846\n",
      "          59     0.9633    0.9604    0.9618       328\n",
      "          60     0.9337    0.9848    0.9586       329\n",
      "          61     0.9274    0.9957    0.9603       231\n",
      "          62     1.0000    0.9880    0.9939       166\n",
      "          63     0.9914    0.9886    0.9900      1053\n",
      "          64     0.8361    0.6220    0.7133       164\n",
      "          65     0.9935    1.0000    0.9967       153\n",
      "          66     0.9749    0.9035    0.9378       601\n",
      "          67     0.9865    0.9744    0.9804       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9674    0.9988    0.9829       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     1.0000    0.9877    0.9938       162\n",
      "          72     0.9901    0.9614    0.9755       725\n",
      "          73     0.9939    0.9759    0.9848       166\n",
      "          74     0.9765    1.0000    0.9881       166\n",
      "          75     0.9947    0.9938    0.9943      3413\n",
      "          76     0.9881    1.0000    0.9940       249\n",
      "          77     0.9509    0.9873    0.9688       157\n",
      "          78     0.9921    0.9843    0.9881       254\n",
      "\n",
      "    accuracy                         0.9705     43442\n",
      "   macro avg     0.9764    0.9646    0.9688     43442\n",
      "weighted avg     0.9709    0.9705    0.9698     43442\n",
      "\n",
      "Testing SVM checkpoint [4/7] - unknown/color_hist...\n",
      "\n",
      "Accuracy on mixed test: 0.8035\n",
      "Time: 97.74s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/color_hist/svm/20260211-114401_5ae17476\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6753    0.8062        77\n",
      "           1     0.7205    0.8780    0.7915      5506\n",
      "           2     0.9496    0.6890    0.7986       164\n",
      "           3     0.8821    0.6991    0.7800      1017\n",
      "           4     0.6435    0.8341    0.7265       645\n",
      "           5     0.8814    0.6753    0.7647        77\n",
      "           6     0.9579    0.6067    0.7429       150\n",
      "           7     0.9925    0.9172    0.9534       145\n",
      "           8     0.9722    0.7000    0.8140       600\n",
      "           9     1.0000    0.7013    0.8244       154\n",
      "          10     0.5986    0.9167    0.7243        96\n",
      "          11     0.8614    0.7839    0.8208       634\n",
      "          12     0.9138    0.7067    0.7970        75\n",
      "          13     0.9068    0.9050    0.9059       484\n",
      "          14     0.9850    0.7892    0.8763       166\n",
      "          15     0.9688    0.6200    0.7561        50\n",
      "          16     0.9598    0.7137    0.8186       234\n",
      "          17     0.9380    0.7094    0.8078       320\n",
      "          18     0.7223    0.8678    0.7884      3048\n",
      "          19     0.8819    0.8301    0.8552       153\n",
      "          20     0.6711    0.9217    0.7766       166\n",
      "          21     1.0000    0.7048    0.8269       166\n",
      "          22     0.8919    0.4342    0.5841       304\n",
      "          23     0.7429    0.8005    0.7706      2065\n",
      "          24     0.9813    0.6325    0.7692       166\n",
      "          25     0.9740    0.6356    0.7692       236\n",
      "          26     0.7309    0.9402    0.8224       234\n",
      "          27     0.9868    0.6132    0.7563       243\n",
      "          28     1.0000    0.7078    0.8289       154\n",
      "          29     0.9913    0.6867    0.8114       166\n",
      "          30     0.9281    0.8132    0.8669      1301\n",
      "          31     0.9161    0.8606    0.8875       330\n",
      "          32     0.9841    0.7470    0.8493       166\n",
      "          33     0.8846    0.7325    0.8014       157\n",
      "          34     1.0000    0.9398    0.9689       166\n",
      "          35     0.8232    0.8976    0.8588       166\n",
      "          36     0.9888    0.5641    0.7184       156\n",
      "          37     1.0000    0.6497    0.7876       157\n",
      "          38     0.7760    0.8554    0.8138       166\n",
      "          39     0.8510    0.7788    0.8133       330\n",
      "          40     1.0000    0.8855    0.9393       166\n",
      "          41     1.0000    0.7771    0.8746       166\n",
      "          42     0.9932    0.8735    0.9295       166\n",
      "          43     0.9409    0.7760    0.8505       308\n",
      "          44     0.9620    0.7451    0.8398       102\n",
      "          45     0.8606    0.8554    0.8580       166\n",
      "          46     0.9558    0.7033    0.8103       246\n",
      "          47     1.0000    0.6646    0.7985       164\n",
      "          48     0.8593    0.6017    0.7078       477\n",
      "          49     0.9945    0.6860    0.8119       793\n",
      "          50     0.7571    0.8651    0.8075      1045\n",
      "          51     0.9496    0.8250    0.8829       160\n",
      "          52     0.9615    0.6807    0.7971       404\n",
      "          53     0.8924    0.8494    0.8704       166\n",
      "          54     0.9460    0.6688    0.7836      1597\n",
      "          55     1.0000    0.7500    0.8571        52\n",
      "          56     0.6524    0.8784    0.7487      4087\n",
      "          57     0.9205    0.8373    0.8770       166\n",
      "          58     0.6847    0.8906    0.7742      1846\n",
      "          59     0.9242    0.7439    0.8243       328\n",
      "          60     0.9205    0.7386    0.8196       329\n",
      "          61     0.9636    0.6883    0.8030       231\n",
      "          62     0.8981    0.8494    0.8731       166\n",
      "          63     0.8537    0.8148    0.8338      1053\n",
      "          64     0.6090    0.4939    0.5455       164\n",
      "          65     0.8562    0.8954    0.8754       153\n",
      "          66     0.9850    0.6539    0.7860       601\n",
      "          67     0.9826    0.6912    0.8115       897\n",
      "          68     1.0000    0.7073    0.8286       164\n",
      "          69     0.9703    0.8631    0.9136       833\n",
      "          70     1.0000    0.8537    0.9211       164\n",
      "          71     0.9829    0.7099    0.8244       162\n",
      "          72     0.9544    0.8083    0.8753       725\n",
      "          73     1.0000    0.7771    0.8746       166\n",
      "          74     1.0000    0.8253    0.9043       166\n",
      "          75     0.9058    0.8028    0.8512      3413\n",
      "          76     1.0000    0.8755    0.9336       249\n",
      "          77     0.8537    0.8917    0.8723       157\n",
      "          78     0.5909    0.9213    0.7200       254\n",
      "\n",
      "    accuracy                         0.8035     43442\n",
      "   macro avg     0.9043    0.7665    0.8195     43442\n",
      "weighted avg     0.8311    0.8035    0.8053     43442\n",
      "\n",
      "Testing SVM checkpoint [5/7] - unknown/heavy_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9727\n",
      "Time: 129.11s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/heavy_augmentation/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        77\n",
      "           1     0.9406    0.9786    0.9592      5506\n",
      "           2     1.0000    1.0000    1.0000       164\n",
      "           3     0.9853    0.9902    0.9877      1017\n",
      "           4     0.9664    0.9798    0.9731       645\n",
      "           5     0.9277    1.0000    0.9625        77\n",
      "           6     0.8889    0.6933    0.7790       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9819    0.9933    0.9876       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9529    0.9890    0.9706       634\n",
      "          12     1.0000    1.0000    1.0000        75\n",
      "          13     1.0000    0.9959    0.9979       484\n",
      "          14     0.9865    0.8795    0.9299       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9474    1.0000    0.9730       234\n",
      "          17     0.8716    0.9969    0.9300       320\n",
      "          18     0.9932    1.0000    0.9966      3048\n",
      "          19     0.9207    0.9869    0.9527       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     1.0000    0.9458    0.9721       166\n",
      "          22     0.9309    0.5757    0.7114       304\n",
      "          23     0.9832    0.9903    0.9867      2065\n",
      "          24     1.0000    1.0000    1.0000       166\n",
      "          25     0.9840    0.7797    0.8700       236\n",
      "          26     1.0000    1.0000    1.0000       234\n",
      "          27     0.9954    0.8889    0.9391       243\n",
      "          28     0.9809    1.0000    0.9904       154\n",
      "          29     0.9765    1.0000    0.9881       166\n",
      "          30     0.9985    1.0000    0.9992      1301\n",
      "          31     1.0000    1.0000    1.0000       330\n",
      "          32     1.0000    0.9940    0.9970       166\n",
      "          33     0.9281    0.9873    0.9568       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8659    0.9936    0.9254       156\n",
      "          37     1.0000    0.9936    0.9968       157\n",
      "          38     0.9939    0.9819    0.9879       166\n",
      "          39     0.9880    0.9939    0.9909       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     1.0000    1.0000    1.0000       166\n",
      "          42     1.0000    1.0000    1.0000       166\n",
      "          43     0.9895    0.9221    0.9546       308\n",
      "          44     0.9903    1.0000    0.9951       102\n",
      "          45     1.0000    1.0000    1.0000       166\n",
      "          46     0.9918    0.9797    0.9857       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9755    0.8344    0.8994       477\n",
      "          49     0.9582    0.9546    0.9564       793\n",
      "          50     0.9570    0.9789    0.9678      1045\n",
      "          51     1.0000    1.0000    1.0000       160\n",
      "          52     0.9755    0.9851    0.9803       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9372    0.9631    0.9500      1597\n",
      "          55     1.0000    0.9615    0.9804        52\n",
      "          56     0.9583    0.9391    0.9486      4087\n",
      "          57     0.9822    1.0000    0.9910       166\n",
      "          58     0.9967    0.9957    0.9962      1846\n",
      "          59     0.9726    0.9726    0.9726       328\n",
      "          60     0.9205    0.9848    0.9515       329\n",
      "          61     0.9502    0.9913    0.9703       231\n",
      "          62     1.0000    0.9880    0.9939       166\n",
      "          63     0.9885    0.9839    0.9862      1053\n",
      "          64     0.8387    0.6341    0.7222       164\n",
      "          65     1.0000    1.0000    1.0000       153\n",
      "          66     0.9767    0.9085    0.9414       601\n",
      "          67     0.9798    0.9755    0.9777       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9731    1.0000    0.9864       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     0.9816    0.9877    0.9846       162\n",
      "          72     0.9957    0.9614    0.9782       725\n",
      "          73     1.0000    0.9759    0.9878       166\n",
      "          74     1.0000    1.0000    1.0000       166\n",
      "          75     0.9959    0.9965    0.9962      3413\n",
      "          76     0.9960    1.0000    0.9980       249\n",
      "          77     0.9503    0.9745    0.9623       157\n",
      "          78     0.9921    0.9843    0.9881       254\n",
      "\n",
      "    accuracy                         0.9727     43442\n",
      "   macro avg     0.9774    0.9676    0.9711     43442\n",
      "weighted avg     0.9729    0.9727    0.9721     43442\n",
      "\n",
      "Testing SVM checkpoint [6/7] - unknown/light_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9724\n",
      "Time: 128.22s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/light_augmentation/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9872    1.0000    0.9935        77\n",
      "           1     0.9384    0.9797    0.9586      5506\n",
      "           2     1.0000    1.0000    1.0000       164\n",
      "           3     0.9853    0.9912    0.9882      1017\n",
      "           4     0.9624    0.9922    0.9771       645\n",
      "           5     0.9625    1.0000    0.9809        77\n",
      "           6     0.8545    0.6267    0.7231       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9752    0.9817    0.9784       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9576    0.9984    0.9776       634\n",
      "          12     1.0000    0.9867    0.9933        75\n",
      "          13     1.0000    0.9979    0.9990       484\n",
      "          14     0.9730    0.8675    0.9172       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9551    1.0000    0.9770       234\n",
      "          17     0.8511    1.0000    0.9195       320\n",
      "          18     0.9941    1.0000    0.9971      3048\n",
      "          19     0.9503    1.0000    0.9745       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     0.9937    0.9518    0.9723       166\n",
      "          22     0.8989    0.5559    0.6870       304\n",
      "          23     0.9898    0.9850    0.9874      2065\n",
      "          24     1.0000    1.0000    1.0000       166\n",
      "          25     0.9947    0.8008    0.8873       236\n",
      "          26     1.0000    1.0000    1.0000       234\n",
      "          27     0.9721    0.8601    0.9127       243\n",
      "          28     0.9935    1.0000    0.9968       154\n",
      "          29     0.9765    1.0000    0.9881       166\n",
      "          30     0.9962    0.9985    0.9973      1301\n",
      "          31     0.9970    1.0000    0.9985       330\n",
      "          32     1.0000    1.0000    1.0000       166\n",
      "          33     0.9455    0.9936    0.9689       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8857    0.9936    0.9366       156\n",
      "          37     1.0000    0.9682    0.9838       157\n",
      "          38     1.0000    0.9639    0.9816       166\n",
      "          39     0.9970    0.9939    0.9954       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     1.0000    1.0000    1.0000       166\n",
      "          42     0.9540    1.0000    0.9765       166\n",
      "          43     0.9897    0.9383    0.9633       308\n",
      "          44     0.9533    1.0000    0.9761       102\n",
      "          45     1.0000    1.0000    1.0000       166\n",
      "          46     0.9919    0.9959    0.9939       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9304    0.8407    0.8833       477\n",
      "          49     0.9655    0.9521    0.9587       793\n",
      "          50     0.9528    0.9847    0.9685      1045\n",
      "          51     1.0000    1.0000    1.0000       160\n",
      "          52     0.9757    0.9950    0.9853       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9531    0.9662    0.9596      1597\n",
      "          55     0.9615    0.9615    0.9615        52\n",
      "          56     0.9582    0.9376    0.9478      4087\n",
      "          57     0.9940    0.9940    0.9940       166\n",
      "          58     0.9967    0.9951    0.9959      1846\n",
      "          59     0.9534    0.9360    0.9446       328\n",
      "          60     0.9235    0.9909    0.9560       329\n",
      "          61     0.9339    0.9784    0.9556       231\n",
      "          62     1.0000    0.9880    0.9939       166\n",
      "          63     0.9924    0.9867    0.9895      1053\n",
      "          64     0.8387    0.6341    0.7222       164\n",
      "          65     1.0000    1.0000    1.0000       153\n",
      "          66     0.9733    0.9085    0.9398       601\n",
      "          67     0.9844    0.9844    0.9844       897\n",
      "          68     1.0000    1.0000    1.0000       164\n",
      "          69     0.9754    0.9976    0.9864       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     0.9938    0.9938    0.9938       162\n",
      "          72     0.9929    0.9586    0.9754       725\n",
      "          73     1.0000    0.9940    0.9970       166\n",
      "          74     0.9940    1.0000    0.9970       166\n",
      "          75     0.9950    0.9933    0.9941      3413\n",
      "          76     1.0000    1.0000    1.0000       249\n",
      "          77     0.9691    1.0000    0.9843       157\n",
      "          78     0.9960    0.9882    0.9921       254\n",
      "\n",
      "    accuracy                         0.9724     43442\n",
      "   macro avg     0.9757    0.9668    0.9699     43442\n",
      "weighted avg     0.9725    0.9724    0.9717     43442\n",
      "\n",
      "Testing SVM checkpoint [7/7] - unknown/medium_augmentation...\n",
      "\n",
      "Accuracy on mixed test: 0.9717\n",
      "Time: 128.27s\n",
      "Scenario distribution (actual): {'clean': 26070, 'scenario_A': 6650, 'scenario_B': 6421, 'scenario_C': 4301}\n",
      "Checkpoint: artifacts/checkpoints/fruit360/medium_augmentation/svm\n",
      "\n",
      "Classification report on mixed test (per class and averages):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9872    1.0000    0.9935        77\n",
      "           1     0.9374    0.9766    0.9566      5506\n",
      "           2     1.0000    1.0000    1.0000       164\n",
      "           3     0.9853    0.9882    0.9867      1017\n",
      "           4     0.9517    0.9767    0.9640       645\n",
      "           5     0.9059    1.0000    0.9506        77\n",
      "           6     0.8860    0.6733    0.7652       150\n",
      "           7     1.0000    1.0000    1.0000       145\n",
      "           8     0.9803    0.9933    0.9868       600\n",
      "           9     1.0000    1.0000    1.0000       154\n",
      "          10     1.0000    1.0000    1.0000        96\n",
      "          11     0.9239    0.9953    0.9582       634\n",
      "          12     1.0000    0.9733    0.9865        75\n",
      "          13     1.0000    0.9959    0.9979       484\n",
      "          14     0.9226    0.8614    0.8910       166\n",
      "          15     1.0000    1.0000    1.0000        50\n",
      "          16     0.9630    1.0000    0.9811       234\n",
      "          17     0.8556    1.0000    0.9222       320\n",
      "          18     0.9932    1.0000    0.9966      3048\n",
      "          19     0.9091    0.9804    0.9434       153\n",
      "          20     1.0000    1.0000    1.0000       166\n",
      "          21     1.0000    0.9639    0.9816       166\n",
      "          22     0.9830    0.5691    0.7208       304\n",
      "          23     0.9860    0.9874    0.9867      2065\n",
      "          24     1.0000    1.0000    1.0000       166\n",
      "          25     0.9744    0.8051    0.8817       236\n",
      "          26     1.0000    1.0000    1.0000       234\n",
      "          27     0.9858    0.8601    0.9187       243\n",
      "          28     0.9809    1.0000    0.9904       154\n",
      "          29     0.9708    1.0000    0.9852       166\n",
      "          30     0.9954    0.9992    0.9973      1301\n",
      "          31     0.9970    1.0000    0.9985       330\n",
      "          32     1.0000    0.9880    0.9939       166\n",
      "          33     0.9627    0.9873    0.9748       157\n",
      "          34     1.0000    1.0000    1.0000       166\n",
      "          35     1.0000    1.0000    1.0000       166\n",
      "          36     0.8902    0.9872    0.9362       156\n",
      "          37     1.0000    0.9809    0.9904       157\n",
      "          38     0.9878    0.9759    0.9818       166\n",
      "          39     0.9939    0.9879    0.9909       330\n",
      "          40     1.0000    1.0000    1.0000       166\n",
      "          41     1.0000    1.0000    1.0000       166\n",
      "          42     0.9708    1.0000    0.9852       166\n",
      "          43     0.9965    0.9123    0.9525       308\n",
      "          44     0.9600    0.9412    0.9505       102\n",
      "          45     0.9538    0.9940    0.9735       166\n",
      "          46     1.0000    0.9837    0.9918       246\n",
      "          47     1.0000    1.0000    1.0000       164\n",
      "          48     0.9432    0.8008    0.8662       477\n",
      "          49     0.9670    0.9609    0.9639       793\n",
      "          50     0.9483    0.9837    0.9657      1045\n",
      "          51     1.0000    1.0000    1.0000       160\n",
      "          52     0.9549    0.9950    0.9745       404\n",
      "          53     1.0000    1.0000    1.0000       166\n",
      "          54     0.9536    0.9643    0.9589      1597\n",
      "          55     0.9792    0.9038    0.9400        52\n",
      "          56     0.9591    0.9344    0.9466      4087\n",
      "          57     0.9940    0.9940    0.9940       166\n",
      "          58     0.9973    0.9978    0.9976      1846\n",
      "          59     0.9635    0.9665    0.9650       328\n",
      "          60     0.9467    0.9726    0.9595       329\n",
      "          61     0.9268    0.9870    0.9560       231\n",
      "          62     0.9939    0.9880    0.9909       166\n",
      "          63     0.9923    0.9801    0.9861      1053\n",
      "          64     0.9266    0.6159    0.7399       164\n",
      "          65     1.0000    1.0000    1.0000       153\n",
      "          66     0.9702    0.9218    0.9454       601\n",
      "          67     0.9833    0.9844    0.9838       897\n",
      "          68     0.9939    1.0000    0.9970       164\n",
      "          69     0.9652    1.0000    0.9823       833\n",
      "          70     1.0000    1.0000    1.0000       164\n",
      "          71     0.9815    0.9815    0.9815       162\n",
      "          72     0.9943    0.9683    0.9811       725\n",
      "          73     1.0000    0.9880    0.9939       166\n",
      "          74     1.0000    1.0000    1.0000       166\n",
      "          75     0.9965    0.9982    0.9974      3413\n",
      "          76     0.9960    1.0000    0.9980       249\n",
      "          77     0.9752    1.0000    0.9874       157\n",
      "          78     0.9881    0.9843    0.9862       254\n",
      "\n",
      "    accuracy                         0.9717     43442\n",
      "   macro avg     0.9757    0.9647    0.9684     43442\n",
      "weighted avg     0.9721    0.9717    0.9710     43442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_mixed_scenarios_checkpoint(test_loader, scenario_fns, probs, model_obj, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    model = model_obj[\"model\"]\n",
    "    scaler = model_obj[\"scaler\"]\n",
    "    scenario_names = list(scenario_fns.keys())\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels_idx = []\n",
    "    scenario_counts = {name: 0 for name in scenario_names}\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for imgs, labels_str in test_loader:\n",
    "        imgs_batch = []\n",
    "        labels_idx_batch = []\n",
    "        for img, lbl_str in zip(imgs, labels_str):\n",
    "            r = np.random.rand()\n",
    "            if r < probs[0]:\n",
    "                scenario = scenario_names[0]      # 'clean'\n",
    "            elif r < probs[0] + probs[1]:\n",
    "                scenario = scenario_names[1]      # 'scenario_A'\n",
    "            elif r < probs[0] + probs[1] + probs[2]:\n",
    "                scenario = scenario_names[2]      # 'scenario_B'\n",
    "            else:\n",
    "                scenario = scenario_names[3]      # 'scenario_C'\n",
    "            \n",
    "            scenario_counts[scenario] += 1\n",
    "            x = scenario_fns[scenario](img)\n",
    "            imgs_batch.append(x.unsqueeze(0))\n",
    "            labels_idx_batch.append(test_dataset_32.label_to_idx[lbl_str])\n",
    "        \n",
    "        imgs_batch = torch.cat(imgs_batch, dim=0)\n",
    "        X = imgs_batch.numpy()\n",
    "        feats = color_hist_features(X, bins=COLOR_BINS, img_shape=(3, SIZE, SIZE))\n",
    "        if scaler is not None:\n",
    "            feats = scaler.transform(feats)\n",
    "        preds = model.predict(feats)\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels_idx.extend(labels_idx_batch)\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels_idx = np.array(all_labels_idx)\n",
    "    acc = (all_preds == all_labels_idx).mean()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return acc, elapsed, scenario_counts, all_labels_idx, all_preds\n",
    "\n",
    "probs_distribution = [0.60, 0.15, 0.15, 0.10]\n",
    "\n",
    "for model_name, ckpts in sorted(models.items()):\n",
    "    if model_name not in (\"knn\", \"svm\"):\n",
    "        continue\n",
    "    if not ckpts:\n",
    "        continue\n",
    "    for ckpt_idx, model_obj in enumerate(ckpts, 1):\n",
    "        noise_name = model_obj.get(\"noise\", \"unknown\")\n",
    "        feature_name = model_obj.get(\"feature\", \"unknown\")\n",
    "        print(f\"Testing {model_name.upper()} checkpoint [{ckpt_idx}/{len(ckpts)}] - {feature_name}/{noise_name}...\")\n",
    "        acc_mixed, time_mixed, counts_mixed, y_true_mixed, y_pred_mixed = evaluate_mixed_scenarios_checkpoint(\n",
    "            test_loader_32,\n",
    "            scenarios_mixed,\n",
    "            probs_distribution,\n",
    "            model_obj=model_obj,\n",
    "            seed=RANDOM_STATE,\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nAccuracy on mixed test: {acc_mixed:.4f}\")\n",
    "        print(f\"Time: {time_mixed:.2f}s\")\n",
    "        print(f\"Scenario distribution (actual): {counts_mixed}\")\n",
    "        print(f\"Checkpoint: {model_obj['run_dir']}\")\n",
    "        \n",
    "        labels = sorted(int(i) for i in np.unique(y_true_mixed))\n",
    "        target_names = [str(test_dataset_32.idx_to_label.get(i, i)) for i in labels]\n",
    "        \n",
    "        print(\"\\nClassification report on mixed test (per class and averages):\")\n",
    "        print(classification_report(\n",
    "            y_true_mixed,\n",
    "            y_pred_mixed,\n",
    "            labels=labels,\n",
    "            target_names=target_names,\n",
    "            digits=4,\n",
    "            zero_division=0,\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
