{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9569a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "SIZE = 32\n",
    "VARIETY = False\n",
    "BATCH_SIZE = 100\n",
    "AUG_RATIO = 0.20\n",
    "AUG_DIST = {'A': 0.4, 'B': 0.4, 'C': 0.2}\n",
    "CV_FOLDS = 3\n",
    "TUNING_SUBSET_RATIO = 0.20\n",
    "VARIANCE_TARGETS = [0.80, 0.90, 0.95]\n",
    "C_VALUES = [10, 100]\n",
    "GAMMA_VALUES = [0.01, 0.001] \n",
    "HIST_BINS = 32\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image size: {SIZE}x{SIZE}\")\n",
    "print(f\"  Augmentation ratio: {AUG_RATIO*100}%\")\n",
    "print(f\"  Augmentation dist: {AUG_DIST}\")\n",
    "print(f\"  CV folds: {CV_FOLDS}\")\n",
    "print(f\"  Variance targets: {VARIANCE_TARGETS}\")\n",
    "print(f\"  C values: {C_VALUES}\")\n",
    "print(f\"  Gamma values: {GAMMA_VALUES}\")\n",
    "print(f\"  Histogram bins: {HIST_BINS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c8da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"V2/dataset/fruit360\"\n",
    "TRAIN_DIR = os.path.join(ROOT_DIR, \"Training\")\n",
    "TEST_DIR = os.path.join(ROOT_DIR, \"Test\")\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/fruits-360/fruits-360-100x100\"\n",
    "CLONE_DIR = \"V2/dataset/fruits-360-100x100\"\n",
    "\n",
    "def download_dataset():\n",
    "    os.makedirs(\"V2/dataset\", exist_ok=True)\n",
    "    subprocess.run([\"git\", \"clone\", GITHUB_REPO, CLONE_DIR], check=True)\n",
    "    \n",
    "    os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Training\"), TRAIN_DIR)\n",
    "    shutil.move(os.path.join(CLONE_DIR, \"Test\"), TEST_DIR)\n",
    "    shutil.rmtree(CLONE_DIR, ignore_errors=True)\n",
    "\n",
    "if not os.path.exists(ROOT_DIR):\n",
    "    download_dataset()\n",
    "\n",
    "assert os.path.exists(TRAIN_DIR), f\"{TRAIN_DIR} not found\"\n",
    "assert os.path.exists(TEST_DIR), f\"{TEST_DIR} not found\"\n",
    "print(f\"Dataset ready: {ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fruit360FolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, variety=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.variety = variety\n",
    "        self.samples = []\n",
    "        \n",
    "        for class_name in sorted(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            \n",
    "            label = class_name if self.variety else class_name.split()[0]\n",
    "            \n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith((\".jpg\", \".png\")):\n",
    "                    self.samples.append((os.path.join(class_dir, img_name), label))\n",
    "        \n",
    "        unique_labels = sorted({lbl for _, lbl in self.samples})\n",
    "        self.label_to_idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
    "        self.idx_to_label = {i: lbl for lbl, i in self.label_to_idx.items()}\n",
    "        \n",
    "        print(f\"{os.path.basename(root_dir)}: {len(self.samples)} images, {len(unique_labels)} classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_str = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.label_to_idx[label_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f85f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((SIZE, SIZE)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_full = Fruit360FolderDataset(TRAIN_DIR, transform=transform, variety=VARIETY)\n",
    "test_dataset = Fruit360FolderDataset(TEST_DIR, transform=transform, variety=VARIETY)\n",
    "\n",
    "train_size = int(0.7 * len(train_full))\n",
    "val_size = len(train_full) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    train_full,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_STATE)\n",
    ")\n",
    "\n",
    "print(f\"Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "print(f\"Classes: {len(train_full.label_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e709f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"{len(train_loader)} train batches, {len(val_loader)} val batches, {len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef409d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numpy(loader):\n",
    "    X_list, y_list = [], []\n",
    "    for imgs, labels in loader:\n",
    "        X_list.append(imgs.numpy())\n",
    "        y_list.append(labels.numpy())\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.concatenate(y_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = extract_numpy(train_loader)\n",
    "X_val, y_val = extract_numpy(val_loader)\n",
    "X_test, y_test = extract_numpy(test_loader)\n",
    "\n",
    "print(f\"X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"X_val {X_val.shape}, y_val {y_val.shape}\")\n",
    "print(f\"X_test {X_test.shape}, y_test {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb089a",
   "metadata": {},
   "source": [
    "## Augmentation: Scenario Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b159dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_01(x):\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "def add_color_patches(x, num_patches, color, alpha_range=(0.4, 0.7), size_range=(0.05, 0.15)):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    for _ in range(num_patches):\n",
    "        s = np.random.uniform(size_range[0], size_range[1])\n",
    "        patch_area = s * H * W / 4\n",
    "        r = np.random.uniform(0.5, 1.5)\n",
    "        patch_h = int(math.sqrt(patch_area / r))\n",
    "        patch_w = int(math.sqrt(patch_area * r))\n",
    "        patch_h = max(1, min(H, patch_h))\n",
    "        patch_w = max(1, min(W, patch_w))\n",
    "        top = np.random.randint(0, H - patch_h + 1)\n",
    "        left = np.random.randint(0, W - patch_w + 1)\n",
    "        bottom = top + patch_h\n",
    "        right = left + patch_w\n",
    "        alpha = np.random.uniform(alpha_range[0], alpha_range[1])\n",
    "        patch = out[:, top:bottom, left:right]\n",
    "        blended = alpha * color + (1 - alpha) * patch\n",
    "        out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "def add_occlusion_patch(x, area_ratio=0.1, color=torch.tensor([0.5, 0.5, 0.5]).view(3,1,1), alpha=0.5):\n",
    "    _, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    patch_area = area_ratio * H * W\n",
    "    r = np.random.uniform(0.5, 1.5)\n",
    "    patch_h = int(math.sqrt(patch_area / r))\n",
    "    patch_w = int(math.sqrt(patch_area * r))\n",
    "    patch_h = max(1, min(H, patch_h))\n",
    "    patch_w = max(1, min(W, patch_w))\n",
    "    top = np.random.randint(0, H - patch_h + 1)\n",
    "    left = np.random.randint(0, W - patch_w + 1)\n",
    "    bottom = top + patch_h\n",
    "    right = left + patch_w\n",
    "    patch = out[:, top:bottom, left:right]\n",
    "    blended = alpha * color + (1 - alpha) * patch\n",
    "    out[:, top:bottom, left:right] = blended\n",
    "    return clamp_01(out)\n",
    "\n",
    "color_dirt = torch.tensor([0.3, 0.25, 0.2]).view(3,1,1)\n",
    "color_bruise = torch.tensor([0.25, 0.2, 0.15]).view(3,1,1)\n",
    "\n",
    "def noise_mild(x):\n",
    "    return clamp_01(x + torch.randn_like(x) * 0.025)\n",
    "\n",
    "def dark_mild(x):\n",
    "    return clamp_01(x * 0.65)\n",
    "\n",
    "def overexposed_mild(x):\n",
    "    return clamp_01(x * 1.35)\n",
    "\n",
    "def dirty_mild(x):\n",
    "    return add_color_patches(x, num_patches=2, color=color_dirt, alpha_range=(0.5, 0.8), size_range=(0.03, 0.08))\n",
    "\n",
    "def bruised_mild(x):\n",
    "    return add_color_patches(x, num_patches=1, color=color_bruise, alpha_range=(0.4, 0.7), size_range=(0.03, 0.08))\n",
    "\n",
    "def occlusion_small(x):\n",
    "    return add_occlusion_patch(x, area_ratio=0.10, alpha=0.5)\n",
    "\n",
    "blur_medium = T.GaussianBlur(kernel_size=5, sigma=1.0)\n",
    "\n",
    "def scenario_A(x):\n",
    "    x = blur_medium(x)\n",
    "    x = noise_mild(x)\n",
    "    if np.random.rand() < 0.7:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_B(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = dark_mild(x)\n",
    "    else:\n",
    "        x = overexposed_mild(x)\n",
    "    x = noise_mild(x)\n",
    "    return x\n",
    "\n",
    "def scenario_C(x):\n",
    "    x = occlusion_small(x)\n",
    "    if np.random.rand() < 0.5:\n",
    "        x = bruised_mild(x)\n",
    "    else:\n",
    "        x = dirty_mild(x)\n",
    "    return x\n",
    "\n",
    "scenario_map = {\n",
    "    'A': scenario_A,\n",
    "    'B': scenario_B,\n",
    "    'C': scenario_C,\n",
    "}\n",
    "\n",
    "print(\"Augmentation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e8dd4",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_augment = int(len(X_train) * AUG_RATIO)\n",
    "aug_indices = np.random.choice(len(X_train), n_augment, replace=False)\n",
    "\n",
    "scenario_counts = {'A': 0, 'B': 0, 'C': 0}\n",
    "\n",
    "for aug_idx in aug_indices:\n",
    "    r = np.random.rand()\n",
    "    if r < AUG_DIST['A']:\n",
    "        scenario_name = 'A'\n",
    "    elif r < AUG_DIST['A'] + AUG_DIST['B']:\n",
    "        scenario_name = 'B'\n",
    "    else:\n",
    "        scenario_name = 'C'\n",
    "    \n",
    "    scenario_counts[scenario_name] += 1\n",
    "    \n",
    "    img_tensor = torch.from_numpy(X_train[aug_idx]).float()\n",
    "    aug_img = scenario_map[scenario_name](img_tensor)\n",
    "    X_train[aug_idx] = aug_img.numpy()\n",
    "\n",
    "print(f\"Augmented {n_augment} images ({AUG_RATIO*100}% of training set)\")\n",
    "print(f\"Scenario distribution: {scenario_counts}\")\n",
    "print(f\"  A: {scenario_counts['A']/n_augment*100:.1f}%\")\n",
    "print(f\"  B: {scenario_counts['B']/n_augment*100:.1f}%\")\n",
    "print(f\"  C: {scenario_counts['C']/n_augment*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98c114",
   "metadata": {},
   "source": [
    "## Preprocessing: Color Histogram Features and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87811ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_hist_features(X, bins=HIST_BINS, img_shape=(3, SIZE, SIZE)):\n",
    "    n_samples = X.shape[0]\n",
    "    feats = np.zeros((n_samples, 3 * bins), dtype=np.float32)\n",
    "    bin_edges = np.linspace(0.0, 1.0, bins + 1)\n",
    "    for i in range(n_samples):\n",
    "        img = X[i].reshape(img_shape)\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "        img_hsv = (img * 255.0).astype(np.uint8)\n",
    "        img_hsv = cv2.cvtColor(img_hsv, cv2.COLOR_RGB2HSV)\n",
    "        h, s, v = cv2.split(img_hsv)\n",
    "        hists = []\n",
    "        for channel in (h, s, v):\n",
    "            ch_norm = channel.astype(np.float32) / 255.0\n",
    "            hist, _ = np.histogram(ch_norm.ravel(), bins=bin_edges, density=True)\n",
    "            hists.append(hist)\n",
    "        feats[i] = np.concatenate(hists)\n",
    "    return feats\n",
    "\n",
    "X_train_hist = color_hist_features(X_train, bins=HIST_BINS, img_shape=(3, SIZE, SIZE))\n",
    "X_val_hist = color_hist_features(X_val, bins=HIST_BINS, img_shape=(3, SIZE, SIZE))\n",
    "X_test_hist = color_hist_features(X_test, bins=HIST_BINS, img_shape=(3, SIZE, SIZE))\n",
    "\n",
    "print(f\"Histogram features shape: {X_train_hist.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train_hist)\n",
    "X_val_sc = scaler.transform(X_val_hist)\n",
    "X_test_sc = scaler.transform(X_test_hist)\n",
    "\n",
    "print(f\"Features per sample: {X_train_hist.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")\n",
    "print(\"Standardization done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3488c7d",
   "metadata": {},
   "source": [
    "## PCA: Variance Analysis & Component Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbdc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(n_components=min(X_train_sc.shape), random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_train_sc)\n",
    "\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "cumsum = np.cumsum(explained)\n",
    "\n",
    "components_for_target = {}\n",
    "for t in VARIANCE_TARGETS:\n",
    "    k = np.argmax(cumsum >= t) + 1\n",
    "    components_for_target[t] = k\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(range(1, min(len(explained) + 1, 200)), explained[:199], marker='o', markersize=2, label='Individual variance')\n",
    "ax1.set_xlabel('Component')\n",
    "ax1.set_ylabel('Explained variance ratio')\n",
    "ax1.set_title('PCA Individual Explained Variance (first 200 components)')\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(range(1, len(cumsum) + 1), cumsum, marker='o', markersize=2, label='Cumulative variance')\n",
    "colors = ['orange', 'red', 'green']\n",
    "for (t, k), col in zip(components_for_target.items(), colors):\n",
    "    ax2.axhline(t, color=col, linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(k, color=col, linestyle='--', alpha=0.5)\n",
    "    ax2.scatter(k, cumsum[k-1], color=col, s=60, zorder=5, \n",
    "                label=f'{int(t*100)}%: {k} comp')\n",
    "\n",
    "ax2.set_xlabel('Number of components')\n",
    "ax2.set_ylabel('Cumulative explained variance')\n",
    "ax2.set_title('PCA Cumulative Explained Variance')\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Components needed for each variance target:\")\n",
    "for t, k in components_for_target.items():\n",
    "    print(f\"  {int(t*100)}% variance: {k} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15363629",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: 3-Fold CV on 20% Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub, _, y_train_sub, _ = train_test_split(\n",
    "    X_train_sc, y_train,\n",
    "    train_size=TUNING_SUBSET_RATIO,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_train,\n",
    ")\n",
    "\n",
    "print(f\"Using subset of size {len(X_train_sub)} from {len(X_train_sc)} for tuning\")\n",
    "\n",
    "n_components_candidates = [\n",
    "    components_for_target[t] for t in VARIANCE_TARGETS\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_comp in n_components_candidates:\n",
    "    pca_temp = PCA(n_components=n_comp, random_state=RANDOM_STATE)\n",
    "    X_sub_pca = pca_temp.fit_transform(X_train_sub)\n",
    "    \n",
    "    for C in C_VALUES:\n",
    "        for gamma in GAMMA_VALUES:\n",
    "            svm = SVC(C=C, gamma=gamma, kernel='rbf', random_state=RANDOM_STATE)\n",
    "            scores = cross_val_score(\n",
    "                svm, X_sub_pca, y_train_sub,\n",
    "                cv=CV_FOLDS, scoring='accuracy', n_jobs=-1,\n",
    "            )\n",
    "            mean_score = scores.mean()\n",
    "            results.append({\n",
    "                'n_components': n_comp,\n",
    "                'C': C,\n",
    "                'gamma': gamma,\n",
    "                'cv_accuracy': mean_score,\n",
    "                'cv_std': scores.std(),\n",
    "            })\n",
    "            print(f\"n_comp={n_comp:4d}, C={C:3d}, gamma={gamma:.3f} -> CV acc {mean_score:.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('cv_accuracy', ascending=False)\n",
    "best_config = results_df.iloc[0]\n",
    "\n",
    "print(\"\\nTop 10 configurations:\")\n",
    "print(results_df.head(10).to_string())\n",
    "\n",
    "print(\"\\nBEST CONFIGURATION\")\n",
    "print(f\"n_components: {int(best_config['n_components'])}\")\n",
    "print(f\"C: {best_config['C']}\")\n",
    "print(f\"gamma: {best_config['gamma']}\")\n",
    "print(f\"CV accuracy: {best_config['cv_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4f3c5",
   "metadata": {},
   "source": [
    "## Final Model Training on Full Augmented Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb108ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_comp = int(best_config['n_components'])\n",
    "best_C = best_config['C']\n",
    "best_gamma = best_config['gamma']\n",
    "\n",
    "print(f\"Training final model with best parameters:\")\n",
    "print(f\"  n_components: {best_n_comp}\")\n",
    "print(f\"  C: {best_C}\")\n",
    "print(f\"  gamma: {best_gamma}\")\n",
    "\n",
    "pca_final = PCA(n_components=best_n_comp, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca_final.fit_transform(X_train_sc)\n",
    "\n",
    "svm_final = SVC(C=best_C, gamma=best_gamma, kernel='rbf', random_state=RANDOM_STATE)\n",
    "svm_final.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Final model trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a54a7a",
   "metadata": {},
   "source": [
    "## Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1148901",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_pca = pca_final.transform(X_val_sc)\n",
    "y_val_pred = svm_final.predict(X_val_pca)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968a364",
   "metadata": {},
   "source": [
    "## Test on Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pca_final.transform(X_test_sc)\n",
    "y_test_pred = svm_final.predict(X_test_pca)\n",
    "test_acc_clean = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test accuracy (clean): {test_acc_clean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f08b0",
   "metadata": {},
   "source": [
    "## Test on Realistic Degraded Dataset (Mixed Scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_mixed = {\n",
    "    \"clean\": lambda x: x,\n",
    "    \"scenario_A\": scenario_A,\n",
    "    \"scenario_B\": scenario_B,\n",
    "    \"scenario_C\": scenario_C,\n",
    "}\n",
    "\n",
    "def evaluate_mixed_scenarios(test_loader, scenario_fns, probs, verbose=True):\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    torch.manual_seed(RANDOM_STATE)\n",
    "    \n",
    "    scenario_names = list(scenario_fns.keys())\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    scenario_counts = {name: 0 for name in scenario_names}\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for imgs, labels in test_loader:\n",
    "        imgs_batch = []\n",
    "        for img in imgs:\n",
    "            r = np.random.rand()\n",
    "            if r < probs[0]:\n",
    "                scenario = scenario_names[0]\n",
    "            elif r < probs[0] + probs[1]:\n",
    "                scenario = scenario_names[1]\n",
    "            elif r < probs[0] + probs[1] + probs[2]:\n",
    "                scenario = scenario_names[2]\n",
    "            else:\n",
    "                scenario = scenario_names[3]\n",
    "            \n",
    "            scenario_counts[scenario] += 1\n",
    "            x = scenario_fns[scenario](img)\n",
    "            imgs_batch.append(x.unsqueeze(0))\n",
    "        \n",
    "        imgs_batch = torch.cat(imgs_batch, dim=0)\n",
    "        X = imgs_batch.numpy()\n",
    "        X_hist = color_hist_features(X, bins=HIST_BINS, img_shape=(3, SIZE, SIZE))\n",
    "        X_hist = scaler.transform(X_hist)\n",
    "        X_hist = pca_final.transform(X_hist)\n",
    "        preds = svm_final.predict(X_hist)\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    acc = (all_preds == all_labels).mean()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAccuracy on mixed realistic distribution: {acc:.4f}\")\n",
    "        print(f\"Time: {elapsed:.2f}s\")\n",
    "        print(f\"Scenario distribution (actual): {scenario_counts}\")\n",
    "    \n",
    "    return acc, elapsed, scenario_counts, all_labels, all_preds\n",
    "\n",
    "probs_distribution = [0.60, 0.15, 0.15, 0.10]\n",
    "\n",
    "acc_mixed, time_mixed, counts_mixed, y_true_mixed, y_pred_mixed = evaluate_mixed_scenarios(\n",
    "    test_loader,\n",
    "    scenarios_mixed,\n",
    "    probs_distribution,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573500f",
   "metadata": {},
   "source": [
    "## Additional Test: Alternative Distribution (50/20/20/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_distribution_alt = [0.50, 0.20, 0.20, 0.10]\n",
    "\n",
    "print(\"Testing with alternative distribution (50% clean / 20% A / 20% B / 10% C)...\")\n",
    "acc_mixed_alt, time_mixed_alt, counts_mixed_alt, y_true_mixed_alt, y_pred_mixed_alt = evaluate_mixed_scenarios(\n",
    "    test_loader,\n",
    "    scenarios_mixed,\n",
    "    probs_distribution_alt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Standard (60/15/15/10): {acc_mixed:.4f}\")\n",
    "print(f\"  Alternative (50/20/20/10): {acc_mixed_alt:.4f}\")\n",
    "print(f\"  Difference: {(acc_mixed_alt - acc_mixed):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec14aa08",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = {\n",
    "    'Model': 'PCA + SVM (Augmented)',\n",
    "    'Image Size': f'{SIZE}x{SIZE}',\n",
    "    'Augmentation': f'{AUG_RATIO*100}% (A: {AUG_DIST[\"A\"]*100:.0f}%, B: {AUG_DIST[\"B\"]*100:.0f}%, C: {AUG_DIST[\"C\"]*100:.0f}%)',\n",
    "    'n_components': best_n_comp,\n",
    "    'C': best_C,\n",
    "    'gamma': best_gamma,\n",
    "    'Val Accuracy': f'{val_acc:.4f}',\n",
    "    'Test Accuracy (clean)': f'{test_acc_clean:.4f}',\n",
    "    'Test Accuracy (60/15/15/10)': f'{acc_mixed:.4f}',\n",
    "    'Test Accuracy (50/20/20/10)': f'{acc_mixed_alt:.4f}',\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, val in results_summary.items():\n",
    "    print(f\"{key:.<40} {val}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nDistribution Comparison:\")\n",
    "print(f\"  Standard (60% clean / 15% A / 15% B / 10% C): {acc_mixed:.4f}\")\n",
    "print(f\"  Harder (50% clean / 20% A / 20% B / 10% C):   {acc_mixed_alt:.4f}\")\n",
    "print(f\"  Accuracy drop: {(acc_mixed - acc_mixed_alt):.4f} ({(acc_mixed - acc_mixed_alt)/acc_mixed*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd9a75",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42609476",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"V2/saved_models\", exist_ok=True)\n",
    "\n",
    "suffix = \"_augmented_colorhist_32x32\"\n",
    "\n",
    "joblib.dump(scaler, f\"V2/saved_models/scaler{suffix}.joblib\")\n",
    "joblib.dump(pca_final, f\"V2/saved_models/pca{suffix}.joblib\")\n",
    "joblib.dump(svm_final, f\"V2/saved_models/svm{suffix}.joblib\")\n",
    "\n",
    "print(f\"Models saved to V2/saved_models/:\")\n",
    "print(f\"  - scaler{suffix}.joblib\")\n",
    "print(f\"  - pca{suffix}.joblib\")\n",
    "print(f\"  - svm{suffix}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d53a73",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b831121",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label = test_dataset.idx_to_label\n",
    "target_names = [idx_to_label[i] for i in range(len(idx_to_label))]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Classification Report - Standard Distribution (60/15/15/10)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true_mixed, y_pred_mixed, target_names=target_names, digits=3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Classification Report - Alternative Distribution (50/20/20/10)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_true_mixed_alt, y_pred_mixed_alt, target_names=target_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48557d5",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(28, 12))\n",
    "\n",
    "cm_standard = confusion_matrix(y_true_mixed, y_pred_mixed)\n",
    "disp_standard = ConfusionMatrixDisplay(confusion_matrix=cm_standard, display_labels=target_names)\n",
    "disp_standard.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix - Standard (60/15/15/10)', fontsize=14, fontweight='bold')\n",
    "\n",
    "cm_alt = confusion_matrix(y_true_mixed_alt, y_pred_mixed_alt)\n",
    "disp_alt = ConfusionMatrixDisplay(confusion_matrix=cm_alt, display_labels=target_names)\n",
    "disp_alt.plot(ax=axes[1], cmap='Oranges')\n",
    "axes[1].set_title('Confusion Matrix - Alternative (50/20/20/10)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Summary:\")\n",
    "print(f\"Standard (60/15/15/10) - Correct predictions: {np.trace(cm_standard)} / {cm_standard.sum()}\")\n",
    "print(f\"Alternative (50/20/20/10) - Correct predictions: {np.trace(cm_alt)} / {cm_alt.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
